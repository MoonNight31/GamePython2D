#!/usr/bin/env python3
"""
Entra√Æneur IA avec Curriculum Learning
Apprentissage progressif par √©tapes : Tir ‚Üí Mouvement ‚Üí Survie
"""

import sys
import os
import time
import numpy as np
from typing import Dict, Any

# Ajouter le r√©pertoire src au path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from gamepython2d.ai_trainer import GameAITrainer
from gamepython2d.ai_environment import GameAIEnvironment

class CurriculumLearningTrainer:
    """Entra√Æneur avec apprentissage par curriculum."""
    
    def __init__(self):
        self.trainer = GameAITrainer()
        self.current_stage = 1
        self.stages = {
            1: "üéØ √âTAPE 1: Apprendre √† tirer",
            2: "üèÉ √âTAPE 2: Apprendre √† se d√©placer", 
            3: "üõ°Ô∏è √âTAPE 3: Apprendre √† survivre"
        }
        
        # Crit√®res de passage d'√©tape
        self.stage_criteria = {
            1: {"projectiles_per_episode": 5.0, "episodes_to_check": 10},
            2: {"movement_score": 0.7, "episodes_to_check": 10},
            3: {"survival_time": 1000, "episodes_to_check": 5}
        }
        
        self.stage_history = []
        
    def create_stage_environment(self, stage: int) -> GameAIEnvironment:
        """Cr√©e un environnement adapt√© √† l'√©tape d'apprentissage."""
        env = GameAIEnvironment(render_mode=None)
        
        # Modifier le syst√®me de r√©compenses selon l'√©tape
        env.curriculum_stage = stage
        
        return env
    
    def train_stage(self, stage: int, total_timesteps: int = 50000):
        """Entra√Æne l'IA pour une √©tape sp√©cifique."""
        print(f"\n{'='*60}")
        print(f"{self.stages[stage]}")
        print(f"{'='*60}")
        
        # Configurer l'entra√Æneur
        self.trainer.create_environment(n_envs=4)
        
        # Si c'est la premi√®re √©tape ou on n'a pas de mod√®le, cr√©er nouveau
        if stage == 1 or not hasattr(self.trainer, 'model') or self.trainer.model is None:
            print("üÜï Cr√©ation d'un nouveau mod√®le")
            self.trainer.create_model(learning_rate=0.0005)
        else:
            print("üîÑ Continuation avec le mod√®le existant")
        
        # Patcher le syst√®me de r√©compenses APR√àS la cr√©ation des environnements
        self._patch_reward_system(stage)
        
        # Entra√Æner
        print(f"üöÄ D√©but de l'entra√Ænement - √âtape {stage}")
        print(f"‚è±Ô∏è Timesteps: {total_timesteps:,}")
        
        start_time = time.time()
        self.trainer.train(total_timesteps=total_timesteps, save_freq=10000)
        
        training_time = time.time() - start_time
        print(f"‚úÖ Entra√Ænement termin√© en {training_time:.1f}s")
        
        # Le mod√®le est automatiquement sauvegard√© par train()
        # Cr√©er une copie sp√©cifique √† l'√©tape si n√©cessaire
        if hasattr(self.trainer, 'model') and self.trainer.model:
            stage_path = f"ai_models/curriculum_stage_{stage}"
            self.trainer.model.save(stage_path)
            print(f"üíæ Mod√®le √©tape {stage} sauvegard√© : {stage_path}")
        
        # √âvaluer les performances
        self._evaluate_stage(stage)
        
    def _patch_reward_system(self, stage: int):
        """Modifie le syst√®me de r√©compenses selon l'√©tape."""
        # Stocker les m√©thodes originales si ce n'est pas d√©j√† fait
        if not hasattr(self, '_original_calculate_reward'):
            self._original_calculate_reward = GameAIEnvironment._calculate_reward
        
        # Cr√©er la nouvelle m√©thode selon l'√©tape
        trainer_instance = self  # Capturer l'instance pour la closure
        
        def curriculum_reward_wrapper(env_self):
            if stage == 1:
                return trainer_instance._stage1_reward(env_self)
            elif stage == 2:
                return trainer_instance._stage2_reward(env_self)
            elif stage == 3:
                return trainer_instance._stage3_reward(env_self)
            else:
                return trainer_instance._original_calculate_reward(env_self)
        
        # Appliquer le patch √† la classe
        GameAIEnvironment._calculate_reward = curriculum_reward_wrapper
        print(f"üìä Syst√®me de r√©compenses configur√© pour l'√©tape {stage}")
        
    def _stage1_reward(self, env) -> float:
        """üéØ √âTAPE 1: R√©compenses focalis√©es sur le TIR."""
        reward = 0.0
        
        # OBJECTIF PRINCIPAL : TIRER DES PROJECTILES
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - env.last_projectile_count)
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 10.0  # TR√àS g√©n√©reux
            env.projectiles_fired += projectiles_fired_this_step
        env.last_projectile_count = current_projectile_count
        
        # Bonus pour viser vers les ennemis
        if hasattr(env, 'last_action') and len(env.enemy_spawner.enemies) > 0:
            move_x, move_y, attack_x, attack_y, should_attack = env.last_action
            if should_attack > 0.5:
                # V√©rifier si la direction de tir est vers un ennemi
                closest_enemy = min(env.enemy_spawner.enemies, 
                                  key=lambda e: ((e.rect.centerx - env.player.rect.centerx)**2 + 
                                               (e.rect.centery - env.player.rect.centery)**2)**0.5)
                
                # Direction vers l'ennemi
                dx = closest_enemy.rect.centerx - env.player.rect.centerx
                dy = closest_enemy.rect.centery - env.player.rect.centery
                
                # Normaliser
                length = (dx**2 + dy**2)**0.5
                if length > 0:
                    dx /= length
                    dy /= length
                    
                    # Calculer similarit√© avec direction de tir
                    dot_product = dx * attack_x + dy * attack_y
                    if dot_product > 0.3:  # 45 degr√©s de tol√©rance
                        reward += 5.0  # Bonus pour bien viser
        
        # Survie minimale
        reward += 0.1
        
        # P√©nalit√© mort seulement
        if env.player.health <= 0:
            reward -= 20.0
            
        return reward
    
    def _stage2_reward(self, env) -> float:
        """üèÉ √âTAPE 2: R√©compenses focalis√©es sur le MOUVEMENT."""
        reward = 0.0
        
        # OBJECTIF PRINCIPAL : SE D√âPLACER INTELLIGEMMENT
        if hasattr(env, 'last_action'):
            move_x, move_y, attack_x, attack_y, should_attack = env.last_action
            is_moving = abs(move_x) > 0.1 or abs(move_y) > 0.1
            
            if is_moving:
                reward += 3.0  # Bonus pour bouger
                
                # Bonus pour s'√©loigner des ennemis
                if len(env.enemy_spawner.enemies) > 0:
                    closest_enemy = min(env.enemy_spawner.enemies, 
                                      key=lambda e: ((e.rect.centerx - env.player.rect.centerx)**2 + 
                                                   (e.rect.centery - env.player.rect.centery)**2)**0.5)
                    
                    # Direction pour s'√©loigner
                    dx = env.player.rect.centerx - closest_enemy.rect.centerx
                    dy = env.player.rect.centery - closest_enemy.rect.centery
                    length = (dx**2 + dy**2)**0.5
                    
                    if length > 0:
                        dx /= length
                        dy /= length
                        
                        # R√©compenser mouvement dans la bonne direction
                        dot_product = dx * move_x + dy * move_y
                        if dot_product > 0:
                            reward += dot_product * 2.0
        
        # Garder les acquis de l'√©tape 1 (mais moins important)
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - env.last_projectile_count)
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 2.0  # Moins g√©n√©reux qu'√©tape 1
        env.last_projectile_count = current_projectile_count
        
        # √âviter les bords
        player_x = env.player.rect.centerx
        player_y = env.player.rect.centery
        if (player_x < 50 or player_x > env.screen_width - 50 or 
            player_y < 50 or player_y > env.screen_height - 50):
            reward -= 2.0
            
        # Survie
        reward += 0.2
        
        # P√©nalit√© d√©g√¢ts
        health_lost = env.last_player_health - env.player.health
        if health_lost > 0:
            reward -= health_lost * 2.0
            env.last_player_health = env.player.health
        
        # P√©nalit√© mort
        if env.player.health <= 0:
            reward -= 30.0
            
        return reward
    
    def _stage3_reward(self, env) -> float:
        """üõ°Ô∏è √âTAPE 3: R√©compenses focalis√©es sur la SURVIE."""
        reward = 0.0
        
        # OBJECTIF PRINCIPAL : SURVIVRE LONGTEMPS
        reward += 0.5  # Bonus de survie augment√©
        
        # Bonus pour kills (combinaison tir + survie)
        for enemy in env.enemy_spawner.enemies[:]:
            if enemy.health <= 0:
                # V√©rifier si tu√© par projectile
                killed_by_projectile = False
                for projectile in env.player.projectiles:
                    if projectile.active and projectile.rect.colliderect(enemy.rect):
                        killed_by_projectile = True
                        break
                
                if killed_by_projectile:
                    reward += 15.0  # Gros bonus pour kill actif
                    env.enemies_killed_by_projectiles += 1
        
        # Garder les acquis des √©tapes pr√©c√©dentes
        # Mouvement (√©tape 2)
        if hasattr(env, 'last_action'):
            move_x, move_y, attack_x, attack_y, should_attack = env.last_action
            is_moving = abs(move_x) > 0.1 or abs(move_y) > 0.1
            if is_moving:
                reward += 1.0
        
        # Tir (√©tape 1)
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - env.last_projectile_count)
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 1.0
        env.last_projectile_count = current_projectile_count
        
        # Gestion des d√©g√¢ts (critique pour survie)
        health_lost = env.last_player_health - env.player.health
        if health_lost > 0:
            reward -= health_lost * 5.0  # P√©nalit√© tr√®s forte
            env.last_player_health = env.player.health
        
        # Position tactique
        player_x = env.player.rect.centerx
        player_y = env.player.rect.centery
        
        # √âviter les bords
        min_distance_to_edge = min(player_x, env.screen_width - player_x, 
                                 player_y, env.screen_height - player_y)
        if min_distance_to_edge < 100:
            reward -= 3.0
        else:
            reward += 0.5  # Bonus position s√©curis√©e
        
        # P√©nalit√© mort massive
        if env.player.health <= 0:
            reward -= 100.0
            
        return reward
    
    def _evaluate_stage(self, stage: int):
        """√âvalue les performances de l'√©tape."""
        print(f"\nüìä √âVALUATION √âTAPE {stage}")
        print("-" * 40)
        
        # Cr√©er un environnement simple pour l'√©valuation
        from gamepython2d.ai_environment import GameAIEnvironment
        env = GameAIEnvironment(render_mode=None)
        
        metrics = {
            "projectiles_per_episode": [],
            "survival_times": [],
            "movement_scores": [],
            "kills": []
        }
        
        for episode in range(5):  # R√©duire le nombre d'√©pisodes pour aller plus vite
            obs, _ = env.reset()
            total_projectiles = 0
            total_movement = 0
            step_count = 0
            kills = 0
            last_projectile_count = 0
            
            while step_count < 1000:  # R√©duire la dur√©e max
                action, _ = self.trainer.model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = env.step(action)
                
                # Mesurer projectiles
                current_projectiles = len([p for p in env.player.projectiles if p.active])
                total_projectiles += max(0, current_projectiles - last_projectile_count)
                last_projectile_count = current_projectiles
                
                # Mesurer mouvement
                move_x, move_y = action[0], action[1]
                movement = (move_x**2 + move_y**2)**0.5
                total_movement += movement
                
                step_count += 1
                
                if terminated or truncated:
                    break
            
            metrics["projectiles_per_episode"].append(total_projectiles)
            metrics["survival_times"].append(step_count)
            metrics["movement_scores"].append(total_movement / step_count if step_count > 0 else 0)
            metrics["kills"].append(info.get('enemies_killed_by_projectiles', 0))
        
        # Afficher r√©sultats
        avg_projectiles = np.mean(metrics["projectiles_per_episode"])
        avg_survival = np.mean(metrics["survival_times"])
        avg_movement = np.mean(metrics["movement_scores"])
        avg_kills = np.mean(metrics["kills"])
        
        print(f"üéØ Projectiles/√©pisode: {avg_projectiles:.1f}")
        print(f"‚è±Ô∏è Temps de survie: {avg_survival:.0f} steps")
        print(f"üèÉ Score de mouvement: {avg_movement:.3f}")
        print(f"‚öîÔ∏è Kills/√©pisode: {avg_kills:.1f}")
        
        # V√©rifier crit√®res de passage
        criteria = self.stage_criteria.get(stage, {})
        ready_for_next = True
        
        if "projectiles_per_episode" in criteria:
            if avg_projectiles < criteria["projectiles_per_episode"]:
                ready_for_next = False
                print(f"‚ùå Crit√®re projectiles non atteint: {avg_projectiles:.1f}/{criteria['projectiles_per_episode']}")
        
        if "movement_score" in criteria:
            if avg_movement < criteria["movement_score"]:
                ready_for_next = False
                print(f"‚ùå Crit√®re mouvement non atteint: {avg_movement:.3f}/{criteria['movement_score']}")
        
        if "survival_time" in criteria:
            if avg_survival < criteria["survival_time"]:
                ready_for_next = False
                print(f"‚ùå Crit√®re survie non atteint: {avg_survival:.0f}/{criteria['survival_time']}")
        
        if ready_for_next and stage < 3:
            print(f"‚úÖ PR√äT POUR L'√âTAPE {stage + 1} !")
        elif stage == 3:
            print("üèÜ CURRICULUM TERMIN√â !")
        else:
            print("‚ö†Ô∏è Besoin d'entra√Ænement suppl√©mentaire")
        
        env.close()
        return ready_for_next
    
    def run_full_curriculum(self):
        """Lance l'apprentissage complet par curriculum."""
        print("üéì D√âMARRAGE DU CURRICULUM LEARNING")
        print("="*60)
        
        for stage in [1, 2, 3]:
            self.train_stage(stage, total_timesteps=100000)
            
            print(f"\n{'='*60}")
            
        print("üéâ CURRICULUM LEARNING TERMIN√â !")
        print("Le mod√®le final est disponible dans ai_models/curriculum_stage_3")

def main():
    """Point d'entr√©e principal."""
    print("üéì CURRICULUM LEARNING TRAINER")
    print("Apprentissage progressif : Tir ‚Üí Mouvement ‚Üí Survie")
    print("="*60)
    
    trainer = CurriculumLearningTrainer()
    
    # Demander √† l'utilisateur
    choice = input("Voulez-vous lancer le curriculum complet ? (o/n): ").strip().lower()
    
    if choice in ['o', 'oui', 'y', 'yes']:
        trainer.run_full_curriculum()
    else:
        print("Curriculum annul√©.")

if __name__ == "__main__":
    main()