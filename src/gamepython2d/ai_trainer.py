import os
import numpy as np
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
import matplotlib.pyplot as plt
from typing import Any, Dict
import time

from .ai_environment import GameAIEnvironment

class TrainingCallback(BaseCallback):
    """Callback personnalis√© pour surveiller l'entra√Ænement."""
    
    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.episode_rewards = []
        self.episode_lengths = []
        self.enemies_killed_log = []
        self.survival_times = []
        
    def _on_step(self) -> bool:
        # R√©cup√©rer les infos de l'environnement
        if len(self.locals.get('infos', [])) > 0:
            info = self.locals['infos'][0]
            
            # Si un √©pisode est termin√©
            if self.locals.get('dones', [False])[0]:
                episode_reward = info.get('total_reward', 0)
                survival_time = info.get('survival_time', 0)
                enemies_killed = info.get('enemies_killed', 0)
                
                self.episode_rewards.append(episode_reward)
                self.survival_times.append(survival_time)
                self.enemies_killed_log.append(enemies_killed)
                
                if self.verbose > 0 and len(self.episode_rewards) % 10 == 0:
                    print(f"√âpisode {len(self.episode_rewards)}: "
                          f"R√©compense={episode_reward:.2f}, "
                          f"Survie={survival_time}, "
                          f"Ennemis tu√©s={enemies_killed}")
        
        return True
    
    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques d'entra√Ænement."""
        if not self.episode_rewards:
            return {}
        
        return {
            'episodes_count': len(self.episode_rewards),
            'avg_reward': np.mean(self.episode_rewards[-100:]),  # Moyenne des 100 derniers
            'max_reward': np.max(self.episode_rewards),
            'avg_survival': np.mean(self.survival_times[-100:]),
            'max_survival': np.max(self.survival_times),
            'avg_enemies_killed': np.mean(self.enemies_killed_log[-100:]),
            'max_enemies_killed': np.max(self.enemies_killed_log)
        }

class GameAITrainer:
    """Classe principale pour entra√Æner l'IA."""
    
    def __init__(self, model_name: str = "game_ai_model"):
        self.model_name = model_name
        self.model_dir = "ai_models"
        self.log_dir = "ai_logs"
        
        # Cr√©er les r√©pertoires
        os.makedirs(self.model_dir, exist_ok=True)
        os.makedirs(self.log_dir, exist_ok=True)
        
        self.model = None
        self.env = None
        self.callback = None
        
    def create_environment(self, n_envs: int = 4, render_mode: str = None):
        """Cr√©e l'environnement d'entra√Ænement."""
        def make_env():
            env = GameAIEnvironment(render_mode=render_mode)
            env = Monitor(env, filename=None)  # Pour logging automatique
            return env
        
        if n_envs == 1:
            self.env = DummyVecEnv([make_env])
        else:
            # Environnements parall√®les pour acc√©l√©rer l'entra√Ænement
            self.env = DummyVecEnv([make_env for _ in range(n_envs)])
        
        return self.env
    
    def create_model(self, learning_rate: float = 3e-4, n_steps: int = 2048, 
                    batch_size: int = 64, n_epochs: int = 10):
        """Cr√©e le mod√®le PPO."""
        if self.env is None:
            raise ValueError("L'environnement doit √™tre cr√©√© avant le mod√®le")
        
        # Configuration du mod√®le PPO
        self.model = PPO(
            "MlpPolicy",  # Politique Multi-Layer Perceptron
            self.env,
            learning_rate=learning_rate,
            n_steps=n_steps,
            batch_size=batch_size,
            n_epochs=n_epochs,
            gamma=0.99,  # Facteur de discount
            gae_lambda=0.95,  # Generalized Advantage Estimation
            clip_range=0.2,  # Clipping PPO
            ent_coef=0.01,  # Coefficient d'entropie pour l'exploration
            vf_coef=0.5,  # Coefficient de la value function
            max_grad_norm=0.5,  # Gradient clipping
            tensorboard_log=self.log_dir,
            verbose=1
        )
        
        return self.model
    
    def train(self, total_timesteps: int = 100000, save_freq: int = 10000):
        """Entra√Æne le mod√®le."""
        if self.model is None:
            raise ValueError("Le mod√®le doit √™tre cr√©√© avant l'entra√Ænement")
        
        print(f"ü§ñ D√©but de l'entra√Ænement pour {total_timesteps} timesteps")
        print(f"üìä Logs sauvegard√©s dans : {self.log_dir}")
        print(f"üíæ Mod√®les sauvegard√©s dans : {self.model_dir}")
        
        # Callback pour surveiller l'entra√Ænement
        self.callback = TrainingCallback(verbose=1)
        
        # Callback pour sauvegarder p√©riodiquement
        checkpoint_callback = None
        if save_freq > 0:
            checkpoint_callback = [
                self.callback,
                # Sauvegarde automatique
                EvalCallback(
                    self.env, 
                    best_model_save_path=f"{self.model_dir}/best_{self.model_name}",
                    log_path=f"{self.log_dir}/eval",
                    eval_freq=save_freq,
                    deterministic=True,
                    render=False
                )
            ]
        else:
            checkpoint_callback = self.callback
        
        # Entra√Ænement
        start_time = time.time()
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=checkpoint_callback,
            tb_log_name=self.model_name
        )
        
        training_time = time.time() - start_time
        print(f"‚úÖ Entra√Ænement termin√© en {training_time:.2f} secondes")
        
        # Sauvegarder le mod√®le final
        model_path = f"{self.model_dir}/{self.model_name}_final"
        self.model.save(model_path)
        print(f"üíæ Mod√®le final sauvegard√© : {model_path}")
        
        return self.model
    
    def load_model(self, model_path: str = None):
        """Charge un mod√®le pr√©-entra√Æn√©."""
        if model_path is None:
            model_path = f"{self.model_dir}/{self.model_name}_final"
        
        if not os.path.exists(f"{model_path}.zip"):
            raise FileNotFoundError(f"Mod√®le non trouv√© : {model_path}.zip")
        
        self.model = PPO.load(model_path, env=self.env)
        print(f"üìÇ Mod√®le charg√© : {model_path}")
        return self.model
    
    def evaluate(self, n_episodes: int = 10, render: bool = False):
        """√âvalue les performances du mod√®le."""
        if self.model is None:
            raise ValueError("Aucun mod√®le charg√©")
        
        print(f"üéØ √âvaluation sur {n_episodes} √©pisodes...")
        
        # Cr√©er un environnement de test
        test_env = GameAIEnvironment(render_mode="human" if render else None)
        
        episode_rewards = []
        episode_lengths = []
        enemies_killed_list = []
        survival_times = []
        
        for episode in range(n_episodes):
            obs, _ = test_env.reset()
            episode_reward = 0
            episode_length = 0
            done = False
            
            while not done:
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = test_env.step(action)
                episode_reward += reward
                episode_length += 1
                done = terminated or truncated
                
                if render:
                    test_env.render()
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            enemies_killed_list.append(info.get('enemies_killed', 0))
            survival_times.append(info.get('survival_time', 0))
            
            print(f"√âpisode {episode + 1}: "
                  f"R√©compense={episode_reward:.2f}, "
                  f"Survie={survival_times[-1]}, "
                  f"Ennemis={enemies_killed_list[-1]}")
        
        test_env.close()
        
        # Statistiques
        stats = {
            'avg_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'max_reward': np.max(episode_rewards),
            'min_reward': np.min(episode_rewards),
            'avg_length': np.mean(episode_lengths),
            'avg_survival': np.mean(survival_times),
            'max_survival': np.max(survival_times),
            'avg_enemies_killed': np.mean(enemies_killed_list),
            'max_enemies_killed': np.max(enemies_killed_list)
        }
        
        print("\nüìä Statistiques d'√©valuation:")
        for key, value in stats.items():
            print(f"  {key}: {value:.2f}")
        
        return stats
    
    def plot_training_progress(self):
        """Affiche les graphiques de progression de l'entra√Ænement."""
        if self.callback is None or not self.callback.episode_rewards:
            print("Aucune donn√©e d'entra√Ænement √† afficher")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Progression de l\'entra√Ænement IA', fontsize=16)
        
        episodes = range(len(self.callback.episode_rewards))
        
        # R√©compenses par √©pisode
        axes[0, 0].plot(episodes, self.callback.episode_rewards)
        axes[0, 0].set_title('R√©compenses par √©pisode')
        axes[0, 0].set_xlabel('√âpisode')
        axes[0, 0].set_ylabel('R√©compense')
        axes[0, 0].grid(True)
        
        # Temps de survie
        axes[0, 1].plot(episodes, self.callback.survival_times, color='green')
        axes[0, 1].set_title('Temps de survie par √©pisode')
        axes[0, 1].set_xlabel('√âpisode')
        axes[0, 1].set_ylabel('Temps de survie')
        axes[0, 1].grid(True)
        
        # Ennemis tu√©s
        axes[1, 0].plot(episodes, self.callback.enemies_killed_log, color='red')
        axes[1, 0].set_title('Ennemis tu√©s par √©pisode')
        axes[1, 0].set_xlabel('√âpisode')
        axes[1, 0].set_ylabel('Ennemis tu√©s')
        axes[1, 0].grid(True)
        
        # Moyenne mobile des r√©compenses
        window_size = min(50, len(self.callback.episode_rewards) // 4)
        if window_size > 1:
            moving_avg = np.convolve(self.callback.episode_rewards, 
                                   np.ones(window_size)/window_size, mode='valid')
            axes[1, 1].plot(episodes[:len(moving_avg)], moving_avg, color='purple')
            axes[1, 1].set_title(f'Moyenne mobile des r√©compenses (fen√™tre={window_size})')
            axes[1, 1].set_xlabel('√âpisode')
            axes[1, 1].set_ylabel('R√©compense moyenne')
            axes[1, 1].grid(True)
        
        plt.tight_layout()
        
        # Sauvegarder le graphique
        plot_path = f"{self.log_dir}/training_progress.png"
        plt.savefig(plot_path)
        print(f"üìà Graphiques sauvegard√©s : {plot_path}")
        
        plt.show()
        
        return fig
    
    def close(self):
        """Ferme l'environnement."""
        if self.env:
            self.env.close()

# Fonctions utilitaires pour usage rapide
def quick_train(timesteps: int = 50000, model_name: str = "quick_ai"):
    """Entra√Ænement rapide avec param√®tres par d√©faut."""
    trainer = GameAITrainer(model_name=model_name)
    trainer.create_environment(n_envs=4)
    trainer.create_model()
    trainer.train(total_timesteps=timesteps)
    trainer.evaluate(n_episodes=5)
    trainer.plot_training_progress()
    trainer.close()
    return trainer

def test_pretrained_model(model_path: str, n_episodes: int = 5, render: bool = True):
    """Teste un mod√®le pr√©-entra√Æn√©."""
    trainer = GameAITrainer()
    trainer.create_environment(n_envs=1, render_mode="human" if render else None)
    trainer.load_model(model_path)
    stats = trainer.evaluate(n_episodes=n_episodes, render=render)
    trainer.close()
    return stats