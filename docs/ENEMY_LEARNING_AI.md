# ğŸ§  SystÃ¨me d'Apprentissage par Renforcement des Ennemis

## Vue d'ensemble

Le systÃ¨me d'apprentissage permet aux ennemis d'**apprendre** les meilleures stratÃ©gies contre vous, plutÃ´t que de simplement gagner en statistiques. Chaque ennemi possÃ¨de un "cerveau" qui utilise le **Q-Learning** pour amÃ©liorer son comportement au fil du temps.

## DiffÃ©rence avec le systÃ¨me adaptatif

### ğŸ¤– IA Adaptative (ancien)
- Les ennemis deviennent **plus forts** (PV, vitesse, dÃ©gÃ¢ts)
- Comportement prÃ©programmÃ© basÃ© sur l'intelligence
- RÃ©agit Ã  vos performances globales

### ğŸ§  IA d'Apprentissage (nouveau)
- Les ennemis **apprennent** les meilleures tactiques
- Comportement **Ã©volutif** basÃ© sur l'expÃ©rience
- Partage de connaissances entre tous les ennemis
- AmÃ©lioration continue par essai-erreur

## Comment Ã§a fonctionne

### 1. Q-Learning simplifiÃ©

Chaque ennemi utilise une **Q-table** qui stocke la valeur de chaque action dans chaque situation :

```
Q-Table
â”œâ”€â”€ Ã‰tat: "CLOSE_0_MOVING_HIGH"
â”‚   â”œâ”€â”€ APPROACH: 2.5
â”‚   â”œâ”€â”€ RETREAT: 1.8
â”‚   â”œâ”€â”€ CIRCLE_LEFT: 3.2  â† Meilleure action
â”‚   â””â”€â”€ ...
â””â”€â”€ Ã‰tat: "FAR_4_STATIC_LOW"
    â”œâ”€â”€ RUSH: 4.1  â† Meilleure action
    â””â”€â”€ ...
```

### 2. Les 8 actions disponibles

Chaque ennemi peut choisir parmi 8 comportements :

| Action | Description | Usage |
|--------|-------------|-------|
| **APPROACH** | Approche directe | Action basique |
| **CIRCLE_LEFT** | Tourner autour (sens anti-horaire) | Esquive + pression |
| **CIRCLE_RIGHT** | Tourner autour (sens horaire) | Esquive + pression |
| **RETREAT** | Reculer | Quand en danger |
| **STRAFE_LEFT** | DÃ©placement latÃ©ral gauche | Esquive tout en avanÃ§ant |
| **STRAFE_RIGHT** | DÃ©placement latÃ©ral droit | Esquive tout en avanÃ§ant |
| **ZIGZAG** | Mouvement en zigzag | Esquive des projectiles |
| **RUSH** | Charge rapide | Attaque agressive |

### 3. Ã‰tats du jeu

L'ennemi perÃ§oit la situation selon 4 dimensions :

#### Distance au joueur
- `CLOSE` : < 100 pixels
- `MEDIUM` : 100-250 pixels
- `FAR` : 250-500 pixels
- `VERY_FAR` : > 500 pixels

#### Direction relative (8 secteurs)
- 0 : Est (droite)
- 1 : Nord-Est
- 2 : Nord (haut)
- 3 : Nord-Ouest
- 4 : Ouest (gauche)
- 5 : Sud-Ouest
- 6 : Sud (bas)
- 7 : Sud-Est

#### Mouvement du joueur
- `STATIC` : Immobile (< 50 vitesse)
- `MOVING` : En dÃ©placement

#### SantÃ© du joueur
- `HIGH` : > 70%
- `MED` : 30-70%
- `LOW` : < 30%

**Exemple d'Ã©tat** : `"MEDIUM_2_MOVING_HIGH"` = Ennemi Ã  distance moyenne, au Nord, joueur en mouvement avec beaucoup de PV.

### 4. SystÃ¨me de rÃ©compenses

L'apprentissage se fait via des rÃ©compenses :

#### RÃ©compenses positives âœ…
```python
+10.0  : Toucher le joueur (grosse rÃ©compense !)
+0.1/s : Survivre
+0.5/s : ÃŠtre Ã  distance optimale (100-250px)
```

#### PÃ©nalitÃ©s nÃ©gatives âŒ
```python
-5.0   : ÃŠtre touchÃ© par un projectile
-0.2/s : ÃŠtre trop proche (< 50px, risque)
-10.0  : Mourir (pÃ©nalitÃ© finale)
```

### 5. StratÃ©gie d'exploration vs exploitation

#### Epsilon-greedy
- **Exploration** : Essayer des actions alÃ©atoires (Îµ = 30% au dÃ©but)
- **Exploitation** : Choisir la meilleure action connue (70%)

L'epsilon **diminue progressivement** :
```python
Îµ = 30% â†’ 25% â†’ 20% â†’ ... â†’ 5% (minimum)
```

Au fil du temps, les ennemis explorent moins et exploitent plus leurs connaissances.

### 6. Apprentissage collectif

**MÃ©moire partagÃ©e** : Tous les ennemis partagent la mÃªme Q-table !

```
Ennemi 1 meurt â†’ Met Ã  jour Q-table locale
                â†“
         Fusionne dans Q-table globale (80% ancien + 20% nouveau)
                â†“
Ennemi 2 spawn â†’ HÃ©rite de la Q-table globale
                â†“
         Utilise les connaissances de tous les ennemis prÃ©cÃ©dents
```

**Avantage** : Les ennemis deviennent de plus en plus intelligents Ã  mesure que la partie avance.

## Ã‰quation de mise Ã  jour

Ã‰quation de Bellman pour le Q-Learning :

```
Q(s,a) = Q(s,a) + Î± Ã— [R + Î³ Ã— max(Q(s',a')) - Q(s,a)]
```

OÃ¹ :
- `Q(s,a)` : Valeur actuelle de l'action `a` dans l'Ã©tat `s`
- `Î±` : Taux d'apprentissage (0.1 = 10% de la nouvelle info)
- `R` : RÃ©compense reÃ§ue
- `Î³` : Facteur de discount (0.95 = importance du futur)
- `max(Q(s',a'))` : Meilleure Q-value du prochain Ã©tat

## Progression de l'apprentissage

### Phase 1 : Exploration (0-50 ennemis tuÃ©s)
- Epsilon Ã©levÃ© (30%)
- Beaucoup d'actions alÃ©atoires
- DÃ©couverte des stratÃ©gies

### Phase 2 : Apprentissage (50-200 ennemis)
- Epsilon moyen (15-20%)
- Mix exploration/exploitation
- Convergence vers bonnes stratÃ©gies

### Phase 3 : Expertise (200+ ennemis)
- Epsilon faible (5-10%)
- Comportement optimisÃ©
- Ennemis trÃ¨s prÃ©visibles dans leurs "bonnes" actions

## MÃ©triques affichÃ©es en jeu

Dans le panneau en bas Ã  droite :

### ğŸ§  Apprentissage
- **Ã‰tats : X** : Nombre de situations diffÃ©rentes rencontrÃ©es
- **Explore : Y%** : Pourcentage d'exploration actuel

Plus le nombre d'Ã©tats est Ã©levÃ©, plus les ennemis ont explorÃ© de situations !

## Exemples de stratÃ©gies apprises

### Situation 1 : Joueur faible santÃ©
```
Ã‰tat: "CLOSE_ANY_MOVING_LOW"
Action optimale: RUSH
Raisonnement: Le joueur est faible, charger rapidement maximise les chances de kill
```

### Situation 2 : Joueur pleine santÃ© Ã  distance
```
Ã‰tat: "FAR_ANY_STATIC_HIGH"
Action optimale: APPROACH
Raisonnement: Pas de danger immÃ©diat, approcher pour engager
```

### Situation 3 : Joueur en mouvement, distance moyenne
```
Ã‰tat: "MEDIUM_ANY_MOVING_HIGH"
Action optimale: ZIGZAG
Raisonnement: Esquiver les projectiles tout en approchant
```

### Situation 4 : Ennemi en danger
```
Ã‰tat: "CLOSE_ANY_MOVING_ANY" + got_hit=True
Action optimale: RETREAT
Raisonnement: Prendre des dÃ©gÃ¢ts â†’ Reculer pour survivre
```

## Configuration avancÃ©e

Dans `enemy_learning_ai.py`, vous pouvez ajuster :

```python
# Vitesse d'apprentissage
learning_rate = 0.1      # Alpha (10% de nouvelle info)

# Importance du futur
discount_factor = 0.95   # Gamma (95% de valeur future)

# Exploration
epsilon = 0.3            # 30% d'exploration au dÃ©part
epsilon_decay = 0.995    # DÃ©croissance par Ã©pisode
min_epsilon = 0.05       # 5% minimum (toujours un peu d'exploration)
```

## DiffÃ©rences de comportement observable

### Sans apprentissage (IA adaptative seule)
- Tous les ennemis du mÃªme type se comportent pareil
- PrÃ©visible aprÃ¨s quelques minutes
- DifficultÃ©s via stats uniquement

### Avec apprentissage
- Chaque ennemi peut agir diffÃ©remment (exploration)
- Comportement global s'amÃ©liore avec le temps
- Surprises tactiques (nouvelles stratÃ©gies)
- Les ennemis "apprennent de vos patterns"

## Performance

- **CoÃ»t calcul** : TrÃ¨s faible (1 recherche Q-table par ennemi par frame)
- **MÃ©moire** : CroÃ®t avec les Ã©tats explorÃ©s (typiquement < 1000 Ã©tats)
- **Pas de ML lourd** : Pas de TensorFlow, PyTorch, etc.
- **Pure Python** : Algorithme lÃ©ger et efficace

## Synergies avec l'IA adaptative

Les deux systÃ¨mes fonctionnent **ensemble** :

1. **IA Adaptative** augmente les stats (PV, vitesse, dÃ©gÃ¢ts)
2. **IA d'Apprentissage** optimise le comportement tactique

RÃ©sultat : Ennemis **forts ET intelligents** !

## DÃ©sactiver l'apprentissage

Pour dÃ©sactiver temporairement (mode test) :

Dans `game.py`, commentez cette ligne lors du spawn :
```python
# new_enemy.brain = self.enemy_learning.create_enemy_brain()
```

L'ennemi utilisera alors le comportement adaptatif standard.

## AmÃ©liorations futures possibles

- ğŸ¯ **Apprentissage profond** : Remplacer Q-table par rÃ©seau de neurones
- ğŸ¤ **Coordination** : Ennemis qui apprennent Ã  attaquer en groupe
- ğŸ“Š **Analyse de patterns** : DÃ©tecter les habitudes du joueur
- ğŸ’¾ **Sauvegarde** : Conserver la Q-table entre sessions
- ğŸ† **Variantes** : Ennemis avec diffÃ©rents styles d'apprentissage
