# üß† Deep Q-Network (DQN) pour l'Apprentissage des Ennemis

## Vue d'ensemble

Le syst√®me utilise maintenant un **r√©seau de neurones profond** (Deep Q-Network) au lieu d'une simple Q-table. Les ennemis apprennent via PyTorch et peuvent g√©n√©raliser leurs connaissances √† des situations jamais rencontr√©es.

## Pourquoi DQN > Q-Learning simple ?

### Q-Learning traditionnel ‚ùå
```
√âtat: "MEDIUM_2_MOVING_HIGH" ‚Üí Q-Value pour chaque action
```
- Limit√© aux √©tats **exactement** rencontr√©s
- Ne peut pas g√©n√©raliser
- Croissance exponentielle de la m√©moire

### Deep Q-Network ‚úÖ
```
√âtat: [0.5, 0.3, 0.8, ...] (vecteur continu)
         ‚Üì
   R√©seau de neurones
         ‚Üì
Q-Values pour toutes les actions
```
- **G√©n√©ralisation** : √âtats similaires ‚Üí Actions similaires
- **M√©moire fixe** : Poids du r√©seau (pas de croissance)
- **Apprentissage puissant** : Patterns complexes

## Architecture du r√©seau

### Structure du DQN

```
Input Layer (16 neurones)
    ‚Üì
Dense 128 + ReLU + Dropout(0.2)
    ‚Üì
Dense 128 + ReLU + Dropout(0.2)
    ‚Üì
Dense 64 + ReLU
    ‚Üì
Output Layer (8 neurones, 1 par action)
```

### Taille du mod√®le
- **Param√®tres** : ~22,000 poids entra√Ænables
- **M√©moire** : ~88 KB (tr√®s l√©ger !)
- **Inference** : < 1ms par d√©cision

## Encodage de l'√©tat

Le vecteur d'√©tat contient **16 dimensions** :

```python
[0]     Position relative X (normalis√©e)
[1]     Position relative Y (normalis√©e)
[2]     Distance au joueur (normalis√©e 0-1)
[3-4]   V√©locit√© joueur X, Y (normalis√©e)
[5]     Magnitude vitesse joueur
[6]     Angle vers joueur (normalis√© 0-1)
[7]     Sant√© joueur (ratio 0-1)
[8]     Sant√© ennemi (ratio 0-1)
[9-12]  Distance cat√©goris√©e (one-hot)
        [9]  CLOSE (< 100px)
        [10] MEDIUM (100-250px)
        [11] FAR (250-500px)
        [12] VERY_FAR (> 500px)
[13-14] Mouvement (one-hot)
        [13] MOVING
        [14] STATIC
[15]    Bias (toujours 1.0)
```

### Exemple d'encodage

```python
Situation: 
- Ennemi √† (300, 400)
- Joueur √† (500, 600) en mouvement rapide
- Distance: 283px
- Sant√© joueur: 80%, Sant√© ennemi: 100%

Vecteur d'√©tat:
[0.20, 0.20,  # Position relative
 0.28,        # Distance normalis√©e
 0.5, 0.3,    # V√©locit√© joueur
 0.6,         # Magnitude
 0.43,        # Angle
 0.80, 1.0,   # Sant√©s
 0, 0, 1, 0,  # FAR (one-hot)
 1, 0,        # MOVING (one-hot)
 1.0]         # Bias
```

## Algorithme DQN

### Double DQN

Le syst√®me utilise **deux r√©seaux** pour la stabilit√© :

1. **Policy Network** : R√©seau principal qui apprend
2. **Target Network** : Copie fixe pour calcul des cibles

```python
# Calcul de la Q-value cible
Q_target = Reward + Œ≥ √ó Q_target_net(next_state, best_action)

# Mise √† jour du policy network
Loss = (Q_policy - Q_target)¬≤
```

### Mise √† jour du Target Network

Tous les **100 steps** :
```python
target_net ‚Üê policy_net
```

Pourquoi ? √âvite l'instabilit√© caus√©e par des cibles qui bougent trop vite.

## Replay Buffer

### Concept

Au lieu d'apprendre imm√©diatement, on **stocke** les exp√©riences :

```
Buffer (10,000 exp√©riences max)
‚îú‚îÄ‚îÄ (s‚ÇÄ, a‚ÇÄ, r‚ÇÄ, s‚ÇÅ, done‚ÇÄ)
‚îú‚îÄ‚îÄ (s‚ÇÅ, a‚ÇÅ, r‚ÇÅ, s‚ÇÇ, done‚ÇÅ)
‚îú‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ (s‚Çô, a‚Çô, r‚Çô, s‚Çô‚Çä‚ÇÅ, done‚Çô)
```

### Apprentissage par batch

Tous les **4 steps** :
1. √âchantillonner **64 exp√©riences** al√©atoires
2. Calculer les Q-targets pour le batch
3. Backpropagation sur le batch
4. Mise √† jour des poids

### Avantages

‚úÖ **Brise corr√©lation temporelle** : Exp√©riences m√©lang√©es
‚úÖ **Efficacit√©** : R√©utilise les exp√©riences multiples fois
‚úÖ **Stabilit√©** : Lissage via moyennage

## Strat√©gie d'exploration

### Epsilon-Greedy avec d√©croissance

```python
Œµ_start = 0.30    # 30% exploration au d√©but
Œµ_min   = 0.05    # 5% minimum (toujours un peu)
decay   = 0.9995  # D√©croissance par step

Œµ_new = max(Œµ_min, Œµ_current √ó decay)
```

### √âvolution typique

```
Episode 0:     Œµ = 30% ‚Üí Beaucoup d'exploration
Episode 100:   Œµ = 18% ‚Üí Mix
Episode 500:   Œµ = 10% ‚Üí Principalement exploitation
Episode 2000:  Œµ = 5%  ‚Üí Exploitation maximale
```

## Fonction de r√©compense

### R√©compenses positives
```python
+15.0       Toucher le joueur (√âNORME r√©compense !)
+0.05/s     Survivre
+0.8/s      Distance optimale (150-250px)
+0.4/s      Distance acceptable (100-150px)
+0.2/s      Se rapprocher (si > 100px)
```

### P√©nalit√©s
```python
-8.0        √ätre touch√© par projectile
-0.3/s      Trop proche (< 80px, danger)
-15.0       Mourir (p√©nalit√© finale)
```

### Reward shaping

Les r√©compenses sont **fa√ßonn√©es** pour guider l'apprentissage :
- R√©compenses fr√©quentes pour survie
- R√©compenses pour "bon" positionnement
- Grosse r√©compense finale pour objectif

## Techniques d'optimisation

### 1. Gradient Clipping
```python
torch.nn.utils.clip_grad_norm_(parameters, 1.0)
```
Emp√™che les gradients explosifs.

### 2. Dropout (0.2)
R√©gularisation pour √©viter l'overfitting.

### 3. Huber Loss (Smooth L1)
Plus robuste que MSE aux outliers.

### 4. Xavier Initialization
Initialisation des poids pour convergence plus rapide.

### 5. Adam Optimizer
Learning rate: 0.0005 (conservatif pour stabilit√©)

## Avantages du DQN

| Crit√®re | Q-Learning | DQN |
|---------|------------|-----|
| **G√©n√©ralisation** | ‚ùå Aucune | ‚úÖ Excellente |
| **M√©moire** | üìà Croissante | ‚úÖ Fixe |
| **Performance** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **√âtats continus** | ‚ùå | ‚úÖ |
| **Apprentissage** | Lent | Rapide |
| **Qualit√© finale** | Bonne | Excellente |

## Statistiques affich√©es

Dans le panneau en jeu :

### üß† DQN Learning
- **M√©moire : X** : Nombre d'exp√©riences dans le buffer
- **Œµ : Y%** : Taux d'exploration actuel
- **CPU/CUDA** : Device utilis√© (vert si GPU)

## Utilisation GPU vs CPU

### D√©tection automatique
```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'
```

### Performance

**CPU (Intel i7)** :
- Inference : ~0.5ms
- Training batch : ~10ms
- FPS jeu : ~60 (aucun impact)

**GPU (NVIDIA)** :
- Inference : ~0.1ms
- Training batch : ~2ms
- FPS jeu : ~60 (encore mieux)

**Verdict** : CPU largement suffisant pour ce cas !

## Sauvegarde et chargement

### Sauvegarder le mod√®le entra√Æn√©
```python
enemy_learning.save_model('models/enemy_dqn_best.pth')
```

### Charger un mod√®le pr√©-entra√Æn√©
```python
enemy_learning.load_model('models/enemy_dqn_best.pth')
```

Le fichier sauvegarde :
- Poids du policy network
- Poids du target network
- √âtat de l'optimiseur
- Epsilon actuel
- Nombre d'√©pisodes

## √âvolution de l'apprentissage

### Phase 1 : Collecte initiale (0-500 exp√©riences)
```
Actions: Al√©atoires (exploration pure)
Buffer: Remplissage
Training: Pas encore
```

### Phase 2 : Premiers apprentissages (500-2000 exp)
```
Actions: Mix al√©atoire/r√©seau
Buffer: En cours de remplissage
Training: Commence (instable)
Performance: Am√©lioration rapide
```

### Phase 3 : Convergence (2000-5000 exp)
```
Actions: Principalement r√©seau
Buffer: Plein (anciennes exp √©cras√©es)
Training: R√©gulier (stable)
Performance: Bonne, s'affine
```

### Phase 4 : Ma√Ætrise (5000+ exp)
```
Actions: R√©seau (95% exploitation)
Buffer: Plein, optimis√©
Training: Fine-tuning
Performance: Excellente !
```

## Comparaison visuelle

### Comportement appris typique

#### D√©but (0-50 kills)
```
Ennemi spawne
    ‚Üì
Action al√©atoire (RETREAT)
    ‚Üì
S'√©loigne du joueur (mauvais)
    ‚Üì
Meurt sans rien faire
    ‚Üì
Q-values mises √† jour
```

#### Milieu (200-500 kills)
```
Ennemi spawne
    ‚Üì
R√©seau choisit APPROACH
    ‚Üì
Se rapproche intelligemment
    ‚Üì
√Ä distance moyenne ‚Üí CIRCLE_LEFT
    ‚Üì
Esquive quelques tirs
    ‚Üì
Parfois touche le joueur (+15 reward!)
```

#### Expert (1000+ kills)
```
Ennemi spawne
    ‚Üì
√âvalue la situation (sant√©, distance, v√©locit√©)
    ‚Üì
Si joueur faible: RUSH agressif
Si joueur fort: ZIGZAG + CIRCLE
    ‚Üì
Positionnement optimal (150-250px)
    ‚Üì
Esquive efficace
    ‚Üì
Touche r√©guli√®rement le joueur
```

## M√©triques de performance

### Loss du r√©seau
```
D√©but:   Loss ~5.0  (apprentissage instable)
Milieu:  Loss ~1.5  (convergence)
Expert:  Loss ~0.3  (optimis√©)
```

### R√©compense moyenne
```
D√©but:   -10.0  (meurt vite)
Milieu:  +5.0   (survit, quelques hits)
Expert:  +20.0  (touche souvent le joueur)
```

## Configuration avanc√©e

Dans `enemy_dqn_ai.py` :

```python
# Architecture du r√©seau
hidden_size = 128        # Taille des couches cach√©es
dropout = 0.2            # Taux de dropout

# Hyperparam√®tres
learning_rate = 0.0005   # Taux d'apprentissage
discount_factor = 0.95   # Gamma (importance du futur)
batch_size = 64          # Taille des batchs
buffer_size = 10000      # Taille du replay buffer

# Exploration
epsilon_start = 0.3
epsilon_min = 0.05
epsilon_decay = 0.9995

# Mise √† jour
train_every = 4          # Entra√Æner tous les N steps
update_target = 100      # MAJ target network tous les N steps
```

## Debugging et monitoring

### Indicateurs de bonne sant√©

‚úÖ **Loss qui diminue** progressivement
‚úÖ **Buffer qui se remplit** rapidement
‚úÖ **Epsilon qui d√©cro√Æt** lentement
‚úÖ **R√©compense moyenne croissante**

### Signes de probl√®me

‚ùå **Loss qui explose** ‚Üí Learning rate trop √©lev√©
‚ùå **Loss qui stagne** ‚Üí Pas assez d'exploration
‚ùå **R√©compense qui descend** ‚Üí Fonction de reward mal con√ßue

## Am√©liorations futures possibles

### üöÄ Prioritized Experience Replay
√âchantillonner plus souvent les exp√©riences "surprenantes"

### üéØ Dueling DQN
S√©parer V(s) et A(s,a) pour meilleure estimation

### üîÑ Rainbow DQN
Combiner toutes les am√©liorations (C51, Noisy Nets, etc.)

### üß† Multi-Agent RL
Ennemis qui apprennent √† coop√©rer

### üìä Curriculum Learning
Entra√Æner progressivement sur situations plus difficiles

## Conclusion

Le syst√®me DQN apporte :

‚úÖ **Apprentissage puissant** via r√©seau de neurones
‚úÖ **G√©n√©ralisation** √† des situations in√©dites
‚úÖ **M√©moire efficace** avec replay buffer
‚úÖ **Stabilit√©** via Double DQN
‚úÖ **Performance** en temps r√©el (CPU suffit)
‚úÖ **Sauvegarde** pour r√©utilisation

Les ennemis deviennent **vraiment intelligents** et apprennent des patterns complexes ! üß†üí™
