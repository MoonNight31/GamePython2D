#!/usr/bin/env python3
"""Diagnostic complet du systÃ¨me de rÃ©compenses"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', 'src'))

from gamepython2d.ai_environment import GameAIEnvironment
import numpy as np
import time

def test_reward_system():
    """Test complet du systÃ¨me de rÃ©compenses."""
    print("ğŸ” DIAGNOSTIC COMPLET DES RÃ‰COMPENSES")
    print("=" * 50)
    
    env = GameAIEnvironment()
    
    # Test 1: RÃ©compenses de base
    print("ğŸ“Š Test 1: RÃ©compenses par action (10 steps chacune)")
    action_names = ['IDLE', 'UP', 'DOWN', 'LEFT', 'RIGHT', 'SHOOT_UP', 'SHOOT_DOWN', 'SHOOT_LEFT', 'SHOOT_RIGHT']
    
    # Actions continues: [move_x, move_y, attack_x, attack_y, should_attack]
    actions = [
        np.array([0.0, 0.0, 0.0, 0.0, 0.0]),      # IDLE
        np.array([0.0, -1.0, 0.0, 0.0, 0.0]),     # UP
        np.array([0.0, 1.0, 0.0, 0.0, 0.0]),      # DOWN
        np.array([-1.0, 0.0, 0.0, 0.0, 0.0]),     # LEFT
        np.array([1.0, 0.0, 0.0, 0.0, 0.0]),      # RIGHT
        np.array([0.0, 0.0, 0.0, -1.0, 0.8]),     # SHOOT_UP
        np.array([0.0, 0.0, 0.0, 1.0, 0.8]),      # SHOOT_DOWN
        np.array([0.0, 0.0, -1.0, 0.0, 0.8]),     # SHOOT_LEFT
        np.array([0.0, 0.0, 1.0, 0.0, 0.8])       # SHOOT_RIGHT
    ]
    
    for action_id, action in enumerate(actions):
        env.reset()
        total_reward = 0
        projectiles_created = 0
        
        print(f"\nğŸ® Test action {action_id} ({action_names[action_id]}):")
        
        for step in range(10):
            obs, reward, done, truncated, info = env.step(action)
            total_reward += reward
            
            # Compter les projectiles
            current_projectiles = len(env.player.projectiles)
            if current_projectiles > projectiles_created:
                projectiles_created = current_projectiles
                print(f"   ğŸ’¥ Step {step}: PROJECTILE CRÃ‰Ã‰! Total={current_projectiles}")
            
            print(f"   Step {step}: reward={reward:.3f}, total={total_reward:.3f}")
            
            if done:
                print(f"   ğŸ’€ Mort au step {step}")
                break
        
        print(f"   ğŸ“ˆ BILAN: Total={total_reward:.2f}, Projectiles={projectiles_created}")
    
    # Test 2: Comportement avec ennemis
    print(f"\n\nğŸ¯ Test 2: Interaction avec ennemis")
    env.reset()
    
    # Simuler plusieurs steps pour voir les kills
    for step in range(50):
        # Alterner entre mouvement et tir
        if step % 3 == 0:
            action = np.array([0.0, 0.0, 0.0, -1.0, 0.8])  # Tir vers le haut
            action_name = "SHOOT_UP"
        else:
            action = np.array([0.0, -1.0, 0.0, 0.0, 0.0])  # Mouvement vers le haut
            action_name = "UP"
            
        obs, reward, done, truncated, info = env.step(action)
        
        if reward != 0.1:  # Si diffÃ©rent de la rÃ©compense de survie
            print(f"   Step {step}: Action {action_name}, Reward={reward:.2f}")
        
        if hasattr(env, 'enemies_killed_this_frame') and env.enemies_killed_this_frame > 0:
            print(f"   ğŸ’€ KILL! Step {step}, Ennemis tuÃ©s: {env.enemies_killed_this_frame}")
        
        if len(env.player.projectiles) > 0:
            print(f"   ğŸš€ Projectiles actifs: {len(env.player.projectiles)}")
        
        if done:
            print(f"   ğŸ’€ Fin de partie au step {step}")
            break
    
    # Test 3: VÃ©rifier la logique de rÃ©compense
    print(f"\n\nğŸ§® Test 3: Logique de rÃ©compense")
    
    # Simuler des situations spÃ©cifiques
    scenarios = [
        (np.array([0.0, 0.0, 0.0, -1.0, 0.8]), "Tir vers le haut"),
        (np.array([0.0, 0.0, 0.0, 1.0, 0.8]), "Tir vers le bas"), 
        (np.array([0.0, 0.0, -1.0, 0.0, 0.8]), "Tir vers la gauche"),
        (np.array([0.0, 0.0, 1.0, 0.0, 0.8]), "Tir vers la droite"),
        (np.array([0.0, 0.0, 0.0, 0.0, 0.0]), "Immobile"),
        (np.array([0.0, -1.0, 0.0, 0.0, 0.0]), "Mouvement haut")
    ]
    
    for action, description in scenarios:
        env.reset()
        obs, reward, done, truncated, info = env.step(action)
        
        # Analyser les composantes de la rÃ©compense
        player_pos = (env.player.rect.centerx, env.player.rect.centery)
        near_wall = (player_pos[0] <= 20 or player_pos[0] >= env.screen_width - 20 or
                    player_pos[1] <= 20 or player_pos[1] >= env.screen_height - 20)
        
        is_shooting = action[4] > 0.5  # should_attack > 0.5
        projectiles_count = len(env.player.projectiles)
        
        print(f"   {description}:")
        print(f"      Reward: {reward:.3f}")
        print(f"      Position: {player_pos}")
        print(f"      PrÃ¨s du mur: {near_wall}")
        print(f"      Action de tir: {is_shooting}")
        print(f"      Projectiles: {projectiles_count}")
    
    env.close()

def test_reward_balance():
    """Test l'Ã©quilibre des rÃ©compenses."""
    print(f"\nğŸ¯ TEST D'Ã‰QUILIBRE DES RÃ‰COMPENSES")
    print("=" * 40)
    
    env = GameAIEnvironment()
    
    # Simuler 1000 steps avec diffÃ©rentes stratÃ©gies
    strategies = {
        "Toujours tirer": lambda step: np.array([0.0, 0.0, 1.0 if step % 2 == 0 else -1.0, 0.0, 0.8]),
        "Jamais tirer": lambda step: np.array([0.5 * (1 if step % 4 < 2 else -1), 0.5 * (1 if step % 4 == 1 or step % 4 == 2 else -1), 0.0, 0.0, 0.0]),
        "Mixte": lambda step: np.array([0.2, 0.0, 0.5, 0.0, 0.6]) if step % 5 == 0 else np.array([0.0, 0.3, 0.0, 0.0, 0.0]),
        "Immobile": lambda step: np.array([0.0, 0.0, 0.0, 0.0, 0.0])
    }
    
    for strategy_name, strategy_func in strategies.items():
        env.reset()
        total_reward = 0
        projectiles_fired = 0
        survival_time = 0
        
        print(f"\nğŸ“Š StratÃ©gie: {strategy_name}")
        
        for step in range(1000):
            action = strategy_func(step)
            obs, reward, done, truncated, info = env.step(action)
            
            total_reward += reward
            survival_time = step
            
            if action[4] > 0.5:  # Actions de tir (should_attack > 0.5)
                projectiles_fired += 1
            
            if done:
                break
        
        avg_reward_per_step = total_reward / (survival_time + 1)
        shooting_ratio = projectiles_fired / (survival_time + 1)
        
        print(f"   ğŸ† RÃ©compense totale: {total_reward:.2f}")
        print(f"   ğŸ“ˆ RÃ©compense/step: {avg_reward_per_step:.4f}")
        print(f"   â±ï¸ Survie: {survival_time} steps")
        print(f"   ğŸ’¥ Ratio tir: {shooting_ratio:.2%}")
        
        # Ã‰valuation
        if avg_reward_per_step > 0:
            print(f"   âœ… StratÃ©gie RENTABLE")
        else:
            print(f"   âŒ StratÃ©gie NON-RENTABLE")

if __name__ == "__main__":
    test_reward_system()
    test_reward_balance()