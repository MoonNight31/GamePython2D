#!/usr/bin/env python3
"""
EntraÃ®neur IA - Focus PrÃ©cision de Tir
SystÃ¨me spÃ©cialisÃ© pour amÃ©liorer l'efficacitÃ© des projectiles
"""

import sys
import os
import time
import numpy as np
import pygame

# Ajouter le rÃ©pertoire src au path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', 'src'))

from gamepython2d.ai_trainer import GameAITrainer
from gamepython2d.ai_environment import GameAIEnvironment

class PrecisionTrainer:
    """EntraÃ®neur spÃ©cialisÃ© pour la prÃ©cision de tir."""
    
    def __init__(self):
        self.trainer = GameAITrainer()
        
    def precision_reward_system(self, env) -> float:
        """SystÃ¨me de rÃ©compenses ultra-focalisÃ© sur la prÃ©cision."""
        reward = 0.0
        
        # ğŸ† RÃ‰COMPENSE PRINCIPALE : KILLS ACTIFS (Ã©norme)
        active_kills_this_step = env.enemies_killed_by_projectiles
        if hasattr(env, '_last_active_kills'):
            new_active_kills = active_kills_this_step - env._last_active_kills
            if new_active_kills > 0:
                reward += new_active_kills * 50.0  # Ã‰NORME rÃ©compense
                print(f"ğŸ¯ KILL ACTIF! +{new_active_kills * 50.0} points")
        env._last_active_kills = active_kills_this_step
        
        # ğŸ¯ RÃ©compense pour tir rÃ©ussi
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - env.last_projectile_count)
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 3.0  # Moins gÃ©nÃ©reux qu'avant
            env.projectiles_fired += projectiles_fired_this_step
        
        # ğŸ§­ BONUS PRÃ‰CISION : RÃ©compenser la visÃ©e intelligente
        if hasattr(env, 'last_action') and len(env.enemy_spawner.enemies) > 0:
            move_x, move_y, attack_x, attack_y, should_attack = env.last_action
            
            if should_attack > 0.5:
                # Trouver l'ennemi le plus proche
                closest_enemy = min(env.enemy_spawner.enemies, 
                                  key=lambda e: ((e.rect.centerx - env.player.rect.centerx)**2 + 
                                               (e.rect.centery - env.player.rect.centery)**2)**0.5)
                
                # Distance Ã  l'ennemi
                distance = ((closest_enemy.rect.centerx - env.player.rect.centerx)**2 + 
                           (closest_enemy.rect.centery - env.player.rect.centery)**2)**0.5
                
                # Direction rÃ©elle vers l'ennemi
                if distance > 0:
                    real_dx = (closest_enemy.rect.centerx - env.player.rect.centerx) / distance
                    real_dy = (closest_enemy.rect.centery - env.player.rect.centery) / distance
                    
                    # PrÃ©cision de la visÃ©e (produit scalaire)
                    accuracy = real_dx * attack_x + real_dy * attack_y
                    
                    if accuracy > 0.9:  # TrÃ¨s prÃ©cis (< 25 degrÃ©s)
                        reward += 5.0
                    elif accuracy > 0.7:  # Assez prÃ©cis (< 45 degrÃ©s)
                        reward += 2.0
                    elif accuracy > 0.3:  # Approximatif (< 70 degrÃ©s)
                        reward += 0.5
                    else:
                        # PÃ©nalitÃ© pour viser complÃ¨tement Ã  cÃ´tÃ©
                        reward -= 1.0
                    
                    # Bonus selon la distance (plus dur de toucher de loin)
                    if distance < 100:  # Proche
                        distance_bonus = 1.0
                    elif distance < 200:  # Moyen
                        distance_bonus = 1.5
                    else:  # Loin
                        distance_bonus = 2.0
                    
                    if accuracy > 0.7:
                        reward += distance_bonus
        
        env.last_projectile_count = current_projectile_count
        
        # ğŸš« PÃ‰NALITÃ‰ MASSIVE pour kills passifs (dÃ©courager cette stratÃ©gie)
        passive_kills_this_step = env.enemies_killed_by_collision
        if hasattr(env, '_last_passive_kills'):
            new_passive_kills = passive_kills_this_step - env._last_passive_kills
            if new_passive_kills > 0:
                reward -= new_passive_kills * 10.0  # Forte pÃ©nalitÃ©
                print(f"ğŸ’¥ Kill passif... -{new_passive_kills * 10.0} points")
        env._last_passive_kills = passive_kills_this_step
        
        # ğŸ’¡ Bonus mouvement intelligent (Ã©viter d'Ãªtre statique)
        if hasattr(env, 'last_action'):
            move_x, move_y, attack_x, attack_y, should_attack = env.last_action
            is_moving = abs(move_x) > 0.2 or abs(move_y) > 0.2
            if is_moving:
                reward += 0.5
        
        # ğŸ›¡ï¸ Survie de base
        reward += 0.1
        
        # ğŸ’€ PÃ©nalitÃ© mort
        if env.player.health <= 0:
            reward -= 30.0
        
        # ğŸ©¸ PÃ©nalitÃ© dÃ©gÃ¢ts (encourager l'Ã©vitement)
        health_lost = env.last_player_health - env.player.health
        if health_lost > 0:
            reward -= health_lost * 2.0
            env.last_player_health = env.player.health
        
        return reward
    
    def train_precision(self, total_timesteps: int = 150000):
        """EntraÃ®ne l'IA pour la prÃ©cision de tir."""
        print("ğŸ¯ ENTRAÃNEMENT PRÃ‰CISION DE TIR")
        print("="*50)
        print("Objectifs :")
        print("  1. ğŸ¯ Maximiser les kills par projectiles")
        print("  2. ğŸ§­ AmÃ©liorer la prÃ©cision de visÃ©e")
        print("  3. ğŸš« DÃ©courager les kills par collision")
        print("  4. ğŸƒ Maintenir un mouvement intelligent")
        print()
        
        # CrÃ©er environnement
        self.trainer.create_environment(n_envs=4)
        
        # Patcher le systÃ¨me de rÃ©compenses
        original_calculate_reward = GameAIEnvironment._calculate_reward
        
        def patched_reward(env_self):
            return self.precision_reward_system(env_self)
        
        GameAIEnvironment._calculate_reward = patched_reward
        
        try:
            # Partir du modÃ¨le curriculum ou crÃ©er nouveau
            if os.path.exists("curriculum_models/stage_1_model.zip"):
                print("ğŸ“¦ Chargement du modÃ¨le curriculum Stage 1 comme base...")
                self.trainer.load_model("curriculum_models/stage_1_model")
            elif os.path.exists("ai_models/game_ai_model_final.zip"):
                print("ğŸ“¦ Chargement du modÃ¨le final comme base...")
                self.trainer.load_model("ai_models/game_ai_model_final")
            else:
                print("ğŸ†• CrÃ©ation d'un nouveau modÃ¨le...")
                self.trainer.create_model()
            
            print(f"ğŸš€ DÃ©but de l'entraÃ®nement spÃ©cialisÃ© ({total_timesteps} timesteps)")
            print("â±ï¸ Temps estimÃ© : ~15-20 minutes")
            print()
            
            # Callback pour suivre les progrÃ¨s
            class PrecisionCallback:
                def __init__(self):
                    self.last_info_time = time.time()
                    self.episode_count = 0
                    
                def __call__(self, locals_, globals_):
                    current_time = time.time()
                    if current_time - self.last_info_time > 45:  # Toutes les 45s
                        if 'infos' in locals_ and len(locals_['infos']) > 0:
                            info = locals_['infos'][0]
                            active_kills = info.get('enemies_killed_by_projectiles', 0)
                            passive_kills = info.get('enemies_killed_by_collision', 0)
                            projectiles = info.get('projectiles_fired', 0)
                            
                            ratio = active_kills / max(1, passive_kills)
                            efficiency = active_kills / max(1, projectiles) * 100
                            
                            print(f"ğŸ“Š Actifs: {active_kills}, Passifs: {passive_kills}, " +
                                  f"Ratio: {ratio:.2f}, EfficacitÃ©: {efficiency:.1f}%")
                        self.last_info_time = current_time
                    return True
            
            # EntraÃ®ner
            self.trainer.train(total_timesteps=total_timesteps)
            
            # Sauvegarder
            model_path = "ai_models/precision_shooting_model"
            self.trainer.save_model(model_path)
            print(f"ğŸ’¾ ModÃ¨le de prÃ©cision sauvegardÃ© : {model_path}")
            
        finally:
            # Restaurer le systÃ¨me original
            GameAIEnvironment._calculate_reward = original_calculate_reward
            
        print("âœ… EntraÃ®nement de prÃ©cision terminÃ©!")
        print("ğŸ¯ Testez avec demo_ai.py pour voir l'amÃ©lioration")

def main():
    """Point d'entrÃ©e principal."""
    trainer = PrecisionTrainer()
    trainer.train_precision()

if __name__ == "__main__":
    main()