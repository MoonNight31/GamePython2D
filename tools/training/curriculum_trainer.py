#!/usr/bin/env python3
"""
Entra√Æneur IA avec Curriculum Learning
Apprentissage progressif par √©tapes : Tir ‚Üí Mouvement ‚Üí Survie
"""

import sys
import os
import time
import numpy as np
from typing import Dict, Any

# Ajouter le r√©pertoire src au path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', 'src'))

from gamepython2d.ai_trainer import GameAITrainer
from gamepython2d.ai_environment import GameAIEnvironment

class CurriculumLearningTrainer:
    """Entra√Æneur avec apprentissage par curriculum."""
    
    def __init__(self):
        self.trainer = GameAITrainer()
        self.current_stage = 1
        self.stages = {
            1: "üéØ √âTAPE 1: Apprendre √† tirer",
            2: "ÔøΩ √âTAPE 2: Apprendre √† viser",
            3: "üèÉ √âTAPE 3: Apprendre √† se d√©placer",
            4: "üõ°Ô∏è √âTAPE 4: Apprendre √† survivre",
            5: "üíé √âTAPE 5: Apprendre √† ramasser les orbes d'XP",
            6: "üÉè √âTAPE 6: Apprendre √† s√©lectionner les cartes",
            7: "üèÜ √âTAPE 7: Ma√Ætrise compl√®te avec am√©liorations"
        }
        
        # Crit√®res de passage d'√©tape
        self.stage_criteria = {
            1: {"projectiles_per_episode": 5.0, "episodes_to_check": 10},
            2: {"accuracy_score": 0.3, "episodes_to_check": 10},
            3: {"movement_score": 0.7, "episodes_to_check": 10},
            4: {"survival_time": 1000, "episodes_to_check": 5},
            5: {"xp_orbs_collected": 3.0, "episodes_to_check": 10},
            6: {"cards_selected": 1.0, "episodes_to_check": 5},
            7: {"survival_time": 2000, "kills": 5.0, "episodes_to_check": 5}
        }
        
        self.stage_history = []
        
    def create_stage_environment(self, stage: int) -> GameAIEnvironment:
        """Cr√©e un environnement adapt√© √† l'√©tape d'apprentissage."""
        env = GameAIEnvironment(render_mode=None)
        
        # Modifier le syst√®me de r√©compenses selon l'√©tape
        env.curriculum_stage = stage
        
        return env
    
    def train_stage(self, stage: int, total_timesteps: int = 50000):
        """Entra√Æne l'IA pour une √©tape sp√©cifique."""
        print(f"\n{'='*60}")
        print(f"{self.stages[stage]}")
        print(f"{'='*60}")
        
        # Configurer l'entra√Æneur
        self.trainer.create_environment(n_envs=4)
        
        # Si c'est la premi√®re √©tape ou on n'a pas de mod√®le, cr√©er nouveau
        if stage == 1 or not hasattr(self.trainer, 'model') or self.trainer.model is None:
            print("üÜï Cr√©ation d'un nouveau mod√®le")
            self.trainer.create_model(learning_rate=0.0005)
        else:
            print("üîÑ Continuation avec le mod√®le existant")
        
        # Patcher le syst√®me de r√©compenses APR√àS la cr√©ation des environnements
        self._patch_reward_system(stage)
        
        # Entra√Æner
        print(f"üöÄ D√©but de l'entra√Ænement - √âtape {stage}")
        print(f"‚è±Ô∏è Timesteps: {total_timesteps:,}")
        
        start_time = time.time()
        self.trainer.train(total_timesteps=total_timesteps, save_freq=10000)
        
        training_time = time.time() - start_time
        print(f"‚úÖ Entra√Ænement termin√© en {training_time:.1f}s")
        
        # Le mod√®le est automatiquement sauvegard√© par train()
        # Cr√©er une copie sp√©cifique √† l'√©tape si n√©cessaire
        if hasattr(self.trainer, 'model') and self.trainer.model:
            stage_path = f"ai_models/curriculum_stage_{stage}"
            self.trainer.model.save(stage_path)
            print(f"üíæ Mod√®le √©tape {stage} sauvegard√© : {stage_path}")
        
        # √âvaluer les performances
        self._evaluate_stage(stage)
        
    def _patch_reward_system(self, stage: int):
        """Modifie le syst√®me de r√©compenses selon l'√©tape."""
        # Stocker les m√©thodes originales si ce n'est pas d√©j√† fait
        if not hasattr(self, '_original_calculate_reward'):
            self._original_calculate_reward = GameAIEnvironment._calculate_reward
        
        # Cr√©er la nouvelle m√©thode selon l'√©tape
        trainer_instance = self  # Capturer l'instance pour la closure
        
        def curriculum_reward_wrapper(env_self):
            if stage == 1:
                return trainer_instance._stage1_reward(env_self)
            elif stage == 2:
                return trainer_instance._stage2_reward(env_self)
            elif stage == 3:
                return trainer_instance._stage3_reward(env_self)
            elif stage == 4:
                return trainer_instance._stage4_reward(env_self)
            elif stage == 5:
                return trainer_instance._stage5_reward(env_self)
            elif stage == 6:
                return trainer_instance._stage6_reward(env_self)
            elif stage == 7:
                return trainer_instance._stage7_reward(env_self)
            else:
                return trainer_instance._original_calculate_reward(env_self)
        
        # Appliquer le patch √† la classe
        GameAIEnvironment._calculate_reward = curriculum_reward_wrapper
        print(f"üìä Syst√®me de r√©compenses configur√© pour l'√©tape {stage}")
        
    def _stage1_reward(self, env) -> float:
        """üéØ √âTAPE 1: R√©compenses focalis√©es sur le TIR."""
        reward = 0.0
        
        # OBJECTIF PRINCIPAL : TIRER DES PROJECTILES
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - env.last_projectile_count)
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 10.0  # TR√àS g√©n√©reux
            env.projectiles_fired += projectiles_fired_this_step
        env.last_projectile_count = current_projectile_count
        
        # Bonus pour viser vers les ennemis
        if hasattr(env, 'last_action') and len(env.enemy_spawner.enemies) > 0:
            move_x, move_y, attack_x, attack_y, should_attack = env.last_action
            if should_attack > 0.5:
                # V√©rifier si la direction de tir est vers un ennemi
                closest_enemy = min(env.enemy_spawner.enemies, 
                                  key=lambda e: ((e.rect.centerx - env.player.rect.centerx)**2 + 
                                               (e.rect.centery - env.player.rect.centery)**2)**0.5)
                
                # Direction vers l'ennemi
                dx = closest_enemy.rect.centerx - env.player.rect.centerx
                dy = closest_enemy.rect.centery - env.player.rect.centery
                
                # Normaliser
                length = (dx**2 + dy**2)**0.5
                if length > 0:
                    dx /= length
                    dy /= length
                    
                    # Calculer similarit√© avec direction de tir
                    dot_product = dx * attack_x + dy * attack_y
                    if dot_product > 0.3:  # 45 degr√©s de tol√©rance
                        reward += 5.0  # Bonus pour bien viser
        
        # Survie minimale
        reward += 0.1
        
        # P√©nalit√© mort seulement
        if env.player.health <= 0:
            reward -= 20.0
            
        return reward
    
    def _stage2_reward(self, env) -> float:
        """ÔøΩ √âTAPE 2: R√©compenses focalis√©es sur la VIS√âE."""
        reward = 0.0
        
        # OBJECTIF PRINCIPAL : VISER CORRECTEMENT LES ENNEMIS
        if hasattr(env, 'last_action') and len(env.enemy_spawner.enemies) > 0:
            move_x, move_y, attack_x, attack_y, should_attack = env.last_action
            
            # Trouver l'ennemi le plus proche
            closest_enemy = min(env.enemy_spawner.enemies, 
                              key=lambda e: ((e.rect.centerx - env.player.rect.centerx)**2 + 
                                           (e.rect.centery - env.player.rect.centery)**2)**0.5)
            
            # Direction vers l'ennemi
            dx = closest_enemy.rect.centerx - env.player.rect.centerx
            dy = closest_enemy.rect.centery - env.player.rect.centery
            
            # Normaliser
            length = (dx**2 + dy**2)**0.5
            if length > 0:
                dx /= length
                dy /= length
                
                # Calculer similarit√© avec direction de tir
                dot_product = dx * attack_x + dy * attack_y
                
                # R√©compenser la bonne vis√©e (m√™me sans tirer)
                if dot_product > 0.5:  # Bonne direction
                    reward += dot_product * 8.0  # TR√àS g√©n√©reux pour bien viser
                
                # Bonus √©norme si tire ET vise bien
                if should_attack > 0.5 and dot_product > 0.3:
                    reward += 12.0  # JACKPOT pour tir bien vis√©
        
        # Garder l'acquis de l'√©tape 1 (tirer)
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - env.last_projectile_count)
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 3.0
        env.last_projectile_count = current_projectile_count
        
        # Bonus pour kills (preuve de bonne vis√©e)
        reward += env.enemies_killed_by_projectiles * 20.0
        
        # Survie minimale
        reward += 0.1
        
        # P√©nalit√© mort
        if env.player.health <= 0:
            reward -= 25.0
            
        return reward
    
    def _stage3_reward(self, env) -> float:
        """ÔøΩüèÉ √âTAPE 3: R√©compenses focalis√©es sur le MOUVEMENT."""
        reward = 0.0
        
        # OBJECTIF PRINCIPAL : SE D√âPLACER INTELLIGEMMENT
        if hasattr(env, 'last_action'):
            move_x, move_y, attack_x, attack_y, should_attack = env.last_action
            is_moving = abs(move_x) > 0.1 or abs(move_y) > 0.1
            
            if is_moving:
                reward += 3.0  # Bonus pour bouger
                
                # Bonus pour s'√©loigner des ennemis
                if len(env.enemy_spawner.enemies) > 0:
                    closest_enemy = min(env.enemy_spawner.enemies, 
                                      key=lambda e: ((e.rect.centerx - env.player.rect.centerx)**2 + 
                                                   (e.rect.centery - env.player.rect.centery)**2)**0.5)
                    
                    # Direction pour s'√©loigner
                    dx = env.player.rect.centerx - closest_enemy.rect.centerx
                    dy = env.player.rect.centery - closest_enemy.rect.centery
                    length = (dx**2 + dy**2)**0.5
                    
                    if length > 0:
                        dx /= length
                        dy /= length
                        
                        # R√©compenser mouvement dans la bonne direction
                        dot_product = dx * move_x + dy * move_y
                        if dot_product > 0:
                            reward += dot_product * 2.0
        
        # Garder les acquis pr√©c√©dents (tir et vis√©e)
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - env.last_projectile_count)
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 2.0
        env.last_projectile_count = current_projectile_count
        
        # √âviter les bords
        player_x = env.player.rect.centerx
        player_y = env.player.rect.centery
        if (player_x < 50 or player_x > env.screen_width - 50 or 
            player_y < 50 or player_y > env.screen_height - 50):
            reward -= 2.0
            
        # Survie
        reward += 0.2
        
        # P√©nalit√© d√©g√¢ts
        health_lost = env.last_player_health - env.player.health
        if health_lost > 0:
            reward -= health_lost * 2.0
            env.last_player_health = env.player.health
        
        # P√©nalit√© mort
        if env.player.health <= 0:
            reward -= 30.0
            
        return reward
    
    def _stage4_reward(self, env) -> float:
        """üõ°Ô∏è √âTAPE 4: R√©compenses focalis√©es sur la SURVIE."""
        reward = 0.0
        
        # OBJECTIF PRINCIPAL : SE D√âPLACER INTELLIGEMMENT
        if hasattr(env, 'last_action'):
            move_x, move_y, attack_x, attack_y, should_attack = env.last_action
            is_moving = abs(move_x) > 0.1 or abs(move_y) > 0.1
            
            if is_moving:
                reward += 3.0  # Bonus pour bouger
                
                # Bonus pour s'√©loigner des ennemis
                if len(env.enemy_spawner.enemies) > 0:
                    closest_enemy = min(env.enemy_spawner.enemies, 
                                      key=lambda e: ((e.rect.centerx - env.player.rect.centerx)**2 + 
                                                   (e.rect.centery - env.player.rect.centery)**2)**0.5)
                    
                    # Direction pour s'√©loigner
                    dx = env.player.rect.centerx - closest_enemy.rect.centerx
                    dy = env.player.rect.centery - closest_enemy.rect.centery
                    length = (dx**2 + dy**2)**0.5
                    
                    if length > 0:
                        dx /= length
                        dy /= length
                        
                        # R√©compenser mouvement dans la bonne direction
                        dot_product = dx * move_x + dy * move_y
                        if dot_product > 0:
                            reward += dot_product * 2.0
        
        # Garder les acquis de l'√©tape 1 (mais moins important)
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - env.last_projectile_count)
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 2.0  # Moins g√©n√©reux qu'√©tape 1
        env.last_projectile_count = current_projectile_count
        
        # √âviter les bords
        player_x = env.player.rect.centerx
        player_y = env.player.rect.centery
        if (player_x < 50 or player_x > env.screen_width - 50 or 
            player_y < 50 or player_y > env.screen_height - 50):
            reward -= 2.0
            
        # Survie
        reward += 0.2
        
        # P√©nalit√© d√©g√¢ts
        health_lost = env.last_player_health - env.player.health
        if health_lost > 0:
            reward -= health_lost * 2.0
            env.last_player_health = env.player.health
        
        # P√©nalit√© mort
        if env.player.health <= 0:
            reward -= 30.0
            
        return reward
    
    def _stage4_reward(self, env) -> float:
        """üõ°Ô∏è √âTAPE 4: R√©compenses focalis√©es sur la SURVIE."""
        reward = 0.0
        
        # OBJECTIF PRINCIPAL : SURVIVRE LONGTEMPS
        reward += 0.5  # Bonus de survie augment√©
        
        # Bonus pour kills (combinaison tir + survie)
        reward += env.enemies_killed_by_projectiles * 15.0
        
        # Garder les acquis (tir, vis√©e, mouvement)
        if hasattr(env, 'last_action'):
            move_x, move_y, attack_x, attack_y, should_attack = env.last_action
            is_moving = abs(move_x) > 0.1 or abs(move_y) > 0.1
            if is_moving:
                reward += 1.0
        
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - env.last_projectile_count)
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 1.0
        env.last_projectile_count = current_projectile_count
        
        # Gestion des d√©g√¢ts (critique pour survie)
        health_lost = env.last_player_health - env.player.health
        if health_lost > 0:
            reward -= health_lost * 5.0
            env.last_player_health = env.player.health
        
        # Position tactique
        player_x = env.player.rect.centerx
        player_y = env.player.rect.centery
        
        # √âviter les bords
        min_distance_to_edge = min(player_x, env.screen_width - player_x, 
                                 player_y, env.screen_height - player_y)
        if min_distance_to_edge < 100:
            reward -= 3.0
        else:
            reward += 0.5
        
        # P√©nalit√© mort massive
        if env.player.health <= 0:
            reward -= 100.0
            
        return reward
    
    def _stage5_reward(self, env) -> float:
        """üíé √âTAPE 5: R√©compenses focalis√©es sur la COLLECTE D'ORBES D'XP."""
        reward = 0.0
        
        # OBJECTIF PRINCIPAL : COLLECTER LES ORBES D'XP
        # Compter les orbes collect√©s dans ce step
        if not hasattr(env, 'last_xp_count'):
            env.last_xp_count = env.xp_system.current_xp
        
        xp_gained = env.xp_system.current_xp - env.last_xp_count
        if xp_gained > 0:
            reward += xp_gained * 2.0  # √âNORME r√©compense pour collecter XP
        env.last_xp_count = env.xp_system.current_xp
        
        # R√©compenser le fait de se diriger vers les orbes
        if len(env.xp_orbs) > 0 and hasattr(env, 'last_action'):
            # Trouver l'orbe le plus proche
            player_pos = (env.player.rect.centerx, env.player.rect.centery)
            closest_orb = min(env.xp_orbs,
                            key=lambda orb: ((orb.x - player_pos[0])**2 + 
                                           (orb.y - player_pos[1])**2)**0.5)
            
            # Direction vers l'orbe
            dx = closest_orb.x - player_pos[0]
            dy = closest_orb.y - player_pos[1]
            length = (dx**2 + dy**2)**0.5
            
            if length > 0:
                dx /= length
                dy /= length
                
                # Mouvement vers l'orbe
                move_x, move_y, _, _, _ = env.last_action
                dot_product = dx * move_x + dy * move_y
                
                if dot_product > 0.3:
                    reward += dot_product * 3.0  # Bonus pour aller vers l'orbe
        
        # Garder tous les acquis pr√©c√©dents (mais avec moins de poids)
        reward += env.enemies_killed_by_projectiles * 10.0
        
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - env.last_projectile_count)
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 0.5
        env.last_projectile_count = current_projectile_count
        
        # Survie
        reward += 0.3
        
        # P√©nalit√© d√©g√¢ts
        health_lost = env.last_player_health - env.player.health
        if health_lost > 0:
            reward -= health_lost * 3.0
            env.last_player_health = env.player.health
        
        # P√©nalit√© mort
        if env.player.health <= 0:
            reward -= 80.0
            
        return reward
    
    def _stage6_reward(self, env) -> float:
        """üÉè √âTAPE 6: R√©compenses focalis√©es sur la S√âLECTION DE CARTES."""
        reward = 0.0
        
        # OBJECTIF PRINCIPAL : S√âLECTIONNER DES CARTES STRAT√âGIQUEMENT
        # V√©rifier si une carte a √©t√© s√©lectionn√©e
        if not hasattr(env, 'last_cards_selected'):
            env.last_cards_selected = 0
        
        # Simuler level ups et s√©lection de cartes
        current_level = env.xp_system.level
        if not hasattr(env, 'last_level'):
            env.last_level = current_level
        
        if current_level > env.last_level:
            # Level up! R√©compense massive
            reward += 50.0
            env.last_cards_selected += 1
            env.last_level = current_level
        
        # R√©compenser la collecte d'XP (pour atteindre les level ups)
        xp_gained = env.xp_system.current_xp - getattr(env, 'last_xp_count', env.xp_system.current_xp)
        if xp_gained > 0:
            reward += xp_gained * 1.5
        env.last_xp_count = env.xp_system.current_xp
        
        # Garder tous les acquis
        reward += env.enemies_killed_by_projectiles * 12.0
        
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - env.last_projectile_count)
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 0.8
        env.last_projectile_count = current_projectile_count
        
        # Survie
        reward += 0.4
        
        # P√©nalit√© d√©g√¢ts
        health_lost = env.last_player_health - env.player.health
        if health_lost > 0:
            reward -= health_lost * 4.0
            env.last_player_health = env.player.health
        
        # P√©nalit√© mort
        if env.player.health <= 0:
            reward -= 100.0
            
        return reward
    
    def _stage7_reward(self, env) -> float:
        """üèÜ √âTAPE 7: MA√éTRISE COMPL√àTE - Survie avec am√©liorations."""
        reward = 0.0
        
        # OBJECTIF FINAL : EXCELLENCE DANS TOUS LES DOMAINES
        
        # Survie longue dur√©e
        reward += 0.6
        
        # Kills avec bonus croissant
        reward += env.enemies_killed_by_projectiles * 20.0
        
        # Bonus pour niveau √©lev√© (preuve d'am√©liorations)
        current_level = env.xp_system.level
        if current_level >= 5:
            reward += (current_level - 4) * 10.0  # Bonus exponentiel
        
        # Collecte d'XP
        xp_gained = env.xp_system.current_xp - getattr(env, 'last_xp_count', env.xp_system.current_xp)
        if xp_gained > 0:
            reward += xp_gained * 1.0
        env.last_xp_count = env.xp_system.current_xp
        
        # Maintien des comp√©tences de base
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - env.last_projectile_count)
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 1.5
        env.last_projectile_count = current_projectile_count
        
        # Mouvement tactique
        if hasattr(env, 'last_action'):
            move_x, move_y, _, _, _ = env.last_action
            is_moving = abs(move_x) > 0.1 or abs(move_y) > 0.1
            if is_moving:
                reward += 1.5
        
        # P√©nalit√© d√©g√¢ts tr√®s forte
        health_lost = env.last_player_health - env.player.health
        if health_lost > 0:
            reward -= health_lost * 8.0
            env.last_player_health = env.player.health
        
        # Position tactique optimale
        player_x = env.player.rect.centerx
        player_y = env.player.rect.centery
        min_distance_to_edge = min(player_x, env.screen_width - player_x, 
                                 player_y, env.screen_height - player_y)
        if min_distance_to_edge < 100:
            reward -= 5.0
        elif min_distance_to_edge > 200:
            reward += 1.0
        
        # P√©nalit√© mort MASSIVE
        if env.player.health <= 0:
            reward -= 150.0
            
        return reward
    
    def _evaluate_stage(self, stage: int):
        """√âvalue les performances de l'√©tape."""
        print(f"\nüìä √âVALUATION √âTAPE {stage}")
        print("-" * 40)
        
        # Cr√©er un environnement simple pour l'√©valuation
        from gamepython2d.ai_environment import GameAIEnvironment
        env = GameAIEnvironment(render_mode=None)
        
        metrics = {
            "projectiles_per_episode": [],
            "survival_times": [],
            "movement_scores": [],
            "kills": [],
            "xp_orbs_collected": [],
            "accuracy_scores": [],
            "cards_selected": [],
            "final_levels": []
        }
        
        for episode in range(5):  # R√©duire le nombre d'√©pisodes pour aller plus vite
            obs, _ = env.reset()
            total_projectiles = 0
            total_movement = 0
            step_count = 0
            kills = 0
            last_projectile_count = 0
            xp_collected = 0
            last_xp = 0
            aimed_shots = 0
            total_shots = 0
            initial_level = env.xp_system.level
            
            while step_count < 1000:  # R√©duire la dur√©e max
                action, _ = self.trainer.model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = env.step(action)
                
                # Mesurer projectiles
                current_projectiles = len([p for p in env.player.projectiles if p.active])
                projectiles_fired = max(0, current_projectiles - last_projectile_count)
                total_projectiles += projectiles_fired
                total_shots += projectiles_fired
                last_projectile_count = current_projectiles
                
                # Mesurer pr√©cision de vis√©e (√©tape 2)
                if projectiles_fired > 0 and len(env.enemy_spawner.enemies) > 0:
                    attack_x, attack_y = action[2], action[3]
                    closest_enemy = min(env.enemy_spawner.enemies,
                                      key=lambda e: ((e.rect.centerx - env.player.rect.centerx)**2 + 
                                                   (e.rect.centery - env.player.rect.centery)**2)**0.5)
                    dx = closest_enemy.rect.centerx - env.player.rect.centerx
                    dy = closest_enemy.rect.centery - env.player.rect.centery
                    length = (dx**2 + dy**2)**0.5
                    if length > 0:
                        dx /= length
                        dy /= length
                        dot_product = dx * attack_x + dy * attack_y
                        if dot_product > 0.3:
                            aimed_shots += 1
                
                # Mesurer mouvement
                move_x, move_y = action[0], action[1]
                movement = (move_x**2 + move_y**2)**0.5
                total_movement += movement
                
                # Mesurer collecte d'XP
                current_xp = env.xp_system.current_xp
                if current_xp > last_xp:
                    xp_collected += (current_xp - last_xp)
                last_xp = current_xp
                
                step_count += 1
                
                if terminated or truncated:
                    break
            
            metrics["projectiles_per_episode"].append(total_projectiles)
            metrics["survival_times"].append(step_count)
            metrics["movement_scores"].append(total_movement / step_count if step_count > 0 else 0)
            metrics["kills"].append(info.get('enemies_killed_by_projectiles', 0))
            metrics["xp_orbs_collected"].append(xp_collected / 10.0)  # Normaliser
            metrics["accuracy_scores"].append(aimed_shots / total_shots if total_shots > 0 else 0)
            metrics["cards_selected"].append(env.xp_system.level - initial_level)
            metrics["final_levels"].append(env.xp_system.level)
        
        # Afficher r√©sultats
        avg_projectiles = np.mean(metrics["projectiles_per_episode"])
        avg_survival = np.mean(metrics["survival_times"])
        avg_movement = np.mean(metrics["movement_scores"])
        avg_kills = np.mean(metrics["kills"])
        avg_xp_collected = np.mean(metrics["xp_orbs_collected"])
        avg_accuracy = np.mean(metrics["accuracy_scores"])
        avg_cards = np.mean(metrics["cards_selected"])
        avg_level = np.mean(metrics["final_levels"])
        
        print(f"üéØ Projectiles/√©pisode: {avg_projectiles:.1f}")
        print(f"üé® Pr√©cision vis√©e: {avg_accuracy:.1%}")
        print(f"üèÉ Score de mouvement: {avg_movement:.3f}")
        print(f"‚è±Ô∏è Temps de survie: {avg_survival:.0f} steps")
        print(f"‚öîÔ∏è Kills/√©pisode: {avg_kills:.1f}")
        print(f"üíé Orbes XP collect√©s: {avg_xp_collected:.1f}")
        print(f"üÉè Cartes obtenues: {avg_cards:.1f}")
        print(f"üìä Niveau moyen final: {avg_level:.1f}")
        
        # V√©rifier crit√®res de passage
        criteria = self.stage_criteria.get(stage, {})
        ready_for_next = True
        
        if "projectiles_per_episode" in criteria:
            if avg_projectiles < criteria["projectiles_per_episode"]:
                ready_for_next = False
                print(f"‚ùå Crit√®re projectiles non atteint: {avg_projectiles:.1f}/{criteria['projectiles_per_episode']}")
        
        if "accuracy_score" in criteria:
            if avg_accuracy < criteria["accuracy_score"]:
                ready_for_next = False
                print(f"‚ùå Crit√®re pr√©cision non atteint: {avg_accuracy:.1%}/{criteria['accuracy_score']:.1%}")
        
        if "movement_score" in criteria:
            if avg_movement < criteria["movement_score"]:
                ready_for_next = False
                print(f"‚ùå Crit√®re mouvement non atteint: {avg_movement:.3f}/{criteria['movement_score']}")
        
        if "survival_time" in criteria:
            if avg_survival < criteria["survival_time"]:
                ready_for_next = False
                print(f"‚ùå Crit√®re survie non atteint: {avg_survival:.0f}/{criteria['survival_time']}")
        
        if "xp_orbs_collected" in criteria:
            if avg_xp_collected < criteria["xp_orbs_collected"]:
                ready_for_next = False
                print(f"‚ùå Crit√®re collecte XP non atteint: {avg_xp_collected:.1f}/{criteria['xp_orbs_collected']}")
        
        if "cards_selected" in criteria:
            if avg_cards < criteria["cards_selected"]:
                ready_for_next = False
                print(f"‚ùå Crit√®re cartes non atteint: {avg_cards:.1f}/{criteria['cards_selected']}")
        
        if "kills" in criteria:
            if avg_kills < criteria["kills"]:
                ready_for_next = False
                print(f"‚ùå Crit√®re kills non atteint: {avg_kills:.1f}/{criteria['kills']}")
        
        if ready_for_next and stage < 7:
            print(f"‚úÖ PR√äT POUR L'√âTAPE {stage + 1} !")
        elif stage == 7:
            print("üèÜ CURRICULUM TERMIN√â !")
        else:
            print("‚ö†Ô∏è Besoin d'entra√Ænement suppl√©mentaire")
        
        env.close()
        return ready_for_next
    
    def run_full_curriculum(self):
        """Lance l'apprentissage complet par curriculum."""
        print("üéì D√âMARRAGE DU CURRICULUM LEARNING - 7 √âTAPES")
        print("="*60)
        
        for stage in [1, 2, 3, 4, 5, 6, 7]:
            self.train_stage(stage, total_timesteps=100000)
            
            print(f"\n{'='*60}")
            
        print("üéâ CURRICULUM LEARNING TERMIN√â !")
        print("Le mod√®le final est disponible dans ai_models/curriculum_stage_7")

def main():
    """Point d'entr√©e principal."""
    print("üéì CURRICULUM LEARNING TRAINER - 7 √âTAPES")
    print("Apprentissage progressif :")
    print("  1. üéØ Tir")
    print("  2. üé® Vis√©e")
    print("  3. üèÉ Mouvement")
    print("  4. üõ°Ô∏è Survie")
    print("  5. üíé Collecte d'orbes XP")
    print("  6. üÉè S√©lection de cartes")
    print("  7. üèÜ Ma√Ætrise compl√®te")
    print("="*60)
    
    trainer = CurriculumLearningTrainer()
    
    # Demander √† l'utilisateur
    choice = input("Voulez-vous lancer le curriculum complet ? (o/n): ").strip().lower()
    
    if choice in ['o', 'oui', 'y', 'yes']:
        trainer.run_full_curriculum()
    else:
        print("Curriculum annul√©.")

if __name__ == "__main__":
    main()