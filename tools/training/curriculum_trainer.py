#!/usr/bin/env python3
"""
EntraÃ®neur IA avec Curriculum Learning
Apprentissage progressif par Ã©tapes : Tir â†’ Mouvement â†’ Survie
"""

import sys
import os
import time
import numpy as np
from typing import Dict, Any

# Ajouter le rÃ©pertoire src au path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', 'src'))

from gamepython2d.ai_trainer import GameAITrainer
from gamepython2d.ai_environment import GameAIEnvironment

class CurriculumLearningTrainer:
    """EntraÃ®neur avec apprentissage par curriculum."""
    
    def __init__(self):
        self.trainer = GameAITrainer()
        self.current_stage = 1
        self.stages = {
            1: "ğŸ¯ Ã‰TAPE 1: Apprendre Ã  tirer",
            2: "ğŸ¯ Ã‰TAPE 2: Apprendre Ã  viser",
            3: "ğŸƒ Ã‰TAPE 3: Apprendre Ã  se dÃ©placer",
            4: "ğŸ›¡ï¸ Ã‰TAPE 4: Apprendre Ã  survivre",
            5: "ğŸ’ Ã‰TAPE 5: Apprendre Ã  ramasser les orbes d'XP",
            6: "ğŸƒ Ã‰TAPE 6: Apprendre Ã  sÃ©lectionner les cartes",
            7: "ğŸ† Ã‰TAPE 7: MaÃ®trise complÃ¨te avec amÃ©liorations"
        }
        
        # CritÃ¨res de passage d'Ã©tape
        self.stage_criteria = {
            1: {"projectiles_per_episode": 5.0, "episodes_to_check": 10},
            2: {"accuracy_score": 0.3, "episodes_to_check": 10},
            3: {"movement_score": 0.7, "episodes_to_check": 10},
            4: {"survival_time": 1000, "episodes_to_check": 5},
            5: {"xp_orbs_collected": 3.0, "episodes_to_check": 10},
            6: {"cards_selected": 1.0, "episodes_to_check": 5},
            7: {"survival_time": 2000, "kills": 5.0, "episodes_to_check": 5}
        }
        
        # Timesteps progressifs par Ã©tape (augmentÃ©s de 300% pour meilleur apprentissage)
        self.stage_timesteps = {
            1: 600000,   # Ã‰tape 1 : Tir de base (600k - Ã©tait 150k)
            2: 800000,   # Ã‰tape 2 : VisÃ©e (800k - Ã©tait 200k)
            3: 1000000,  # Ã‰tape 3 : Mouvement (1M - Ã©tait 250k)
            4: 1200000,  # Ã‰tape 4 : Survie (1.2M - Ã©tait 300k)
            5: 1400000,  # Ã‰tape 5 : Collecte XP (1.4M - Ã©tait 350k)
            6: 1200000,  # Ã‰tape 6 : Cartes (1.2M - Ã©tait 300k)
            7: 2000000,  # Ã‰tape 7 : MaÃ®trise complÃ¨te (2M - Ã©tait 500k)
        }
        
        # Nombre de tentatives maximum par Ã©tape
        self.max_retries = 2
        
        self.stage_history = []
        
    def create_stage_environment(self, stage: int) -> GameAIEnvironment:
        """CrÃ©e un environnement adaptÃ© Ã  l'Ã©tape d'apprentissage."""
        env = GameAIEnvironment(render_mode=None)
        
        # Modifier le systÃ¨me de rÃ©compenses selon l'Ã©tape
        env.curriculum_stage = stage
        
        return env
    
    def train_stage(self, stage: int, total_timesteps: int = None):
        """EntraÃ®ne l'IA pour une Ã©tape spÃ©cifique."""
        # Utiliser les timesteps par dÃ©faut si non spÃ©cifiÃ©
        if total_timesteps is None:
            total_timesteps = self.stage_timesteps.get(stage, 100000)
        
        print(f"\n{'='*60}")
        print(f"{self.stages[stage]}")
        print(f"{'='*60}")
        
        # Configurer l'entraÃ®neur
        self.trainer.create_environment(n_envs=30)
        
        # Si c'est la premiÃ¨re Ã©tape ou on n'a pas de modÃ¨le, crÃ©er nouveau
        if stage == 1 or not hasattr(self.trainer, 'model') or self.trainer.model is None:
            print("ğŸ†• CrÃ©ation d'un nouveau modÃ¨le")
            # Batch size optimal pour 30 envs avec n_steps=2048
            # 30 * 2048 = 61440, diviseurs parfaits: 480, 512, 640
            self.trainer.create_model(learning_rate=0.0005, batch_size=512)
        else:
            print("ğŸ”„ Continuation avec le modÃ¨le existant")
        
        # Patcher le systÃ¨me de rÃ©compenses APRÃˆS la crÃ©ation des environnements
        self._patch_reward_system(stage)
        
        # EntraÃ®ner
        print(f"ğŸš€ DÃ©but de l'entraÃ®nement - Ã‰tape {stage}")
        print(f"â±ï¸ Timesteps: {total_timesteps:,}")
        
        start_time = time.time()
        self.trainer.train(total_timesteps=total_timesteps, save_freq=10000)
        
        training_time = time.time() - start_time
        print(f"âœ… EntraÃ®nement terminÃ© en {training_time:.1f}s ({training_time/60:.1f} minutes)")
        
        # Le modÃ¨le est automatiquement sauvegardÃ© par train()
        # CrÃ©er une copie spÃ©cifique Ã  l'Ã©tape si nÃ©cessaire
        if hasattr(self.trainer, 'model') and self.trainer.model:
            stage_path = f"ai_models/curriculum_stage_{stage}"
            self.trainer.model.save(stage_path)
            print(f"ğŸ’¾ ModÃ¨le Ã©tape {stage} sauvegardÃ© : {stage_path}")
        
        # Ã‰valuer les performances
        return self._evaluate_stage(stage)
    
    def train_stage_with_validation(self, stage: int):
        """EntraÃ®ne une Ã©tape avec validation stricte et retry si Ã©chec."""
        print(f"\n{'='*70}")
        print(f"ğŸ“ ENTRAÃNEMENT AVEC VALIDATION - {self.stages[stage]}")
        print(f"{'='*70}")
        print(f"ğŸ“Š Timesteps prÃ©vus: {self.stage_timesteps[stage]:,}")
        print(f"ğŸ”„ Tentatives maximum: {self.max_retries}")
        print(f"{'='*70}\n")
        
        for attempt in range(1, self.max_retries + 1):
            print(f"\n{'ğŸ”¥'*30}")
            print(f"ğŸ”„ TENTATIVE {attempt}/{self.max_retries} - Ã‰tape {stage}")
            print(f"{'ğŸ”¥'*30}\n")
            
            # EntraÃ®ner l'Ã©tape
            ready = self.train_stage(stage)
            
            if ready:
                print(f"\n{'âœ…'*30}")
                print(f"âœ… Ã‰TAPE {stage} VALIDÃ‰E avec succÃ¨s !")
                print(f"{'âœ…'*30}\n")
                return True
            else:
                if attempt < self.max_retries:
                    print(f"\n{'âš ï¸'*30}")
                    print(f"âš ï¸ Ã‰tape {stage} NON VALIDÃ‰E - RÃ©-entraÃ®nement avec 50% de timesteps supplÃ©mentaires...")
                    print(f"{'âš ï¸'*30}\n")
                    # Augmenter les timesteps de 50% pour la prochaine tentative
                    self.stage_timesteps[stage] = int(self.stage_timesteps[stage] * 1.5)
                else:
                    print(f"\n{'âŒ'*30}")
                    print(f"âŒ ATTENTION: Ã‰tape {stage} NON VALIDÃ‰E aprÃ¨s {self.max_retries} tentatives")
                    print(f"âŒ Passage forcÃ© Ã  l'Ã©tape suivante...")
                    print(f"{'âŒ'*30}\n")
        
        return False
        
    def _patch_reward_system(self, stage: int):
        """Modifie le systÃ¨me de rÃ©compenses selon l'Ã©tape."""
        # Stocker les mÃ©thodes originales si ce n'est pas dÃ©jÃ  fait
        if not hasattr(self, '_original_calculate_reward'):
            self._original_calculate_reward = GameAIEnvironment._calculate_reward
        
        # CrÃ©er la nouvelle mÃ©thode selon l'Ã©tape
        trainer_instance = self  # Capturer l'instance pour la closure
        
        def curriculum_reward_wrapper(env_self):
            if stage == 1:
                return trainer_instance._stage1_reward(env_self)
            elif stage == 2:
                return trainer_instance._stage2_reward(env_self)
            elif stage == 3:
                return trainer_instance._stage3_reward(env_self)
            elif stage == 4:
                return trainer_instance._stage4_reward(env_self)
            elif stage == 5:
                return trainer_instance._stage5_reward(env_self)
            elif stage == 6:
                return trainer_instance._stage6_reward(env_self)
            elif stage == 7:
                return trainer_instance._stage7_reward(env_self)
            else:
                return trainer_instance._original_calculate_reward(env_self)
        
        # Appliquer le patch Ã  la classe
        GameAIEnvironment._calculate_reward = curriculum_reward_wrapper
        print(f"ğŸ“Š SystÃ¨me de rÃ©compenses configurÃ© pour l'Ã©tape {stage}")
        
    def _stage1_reward(self, env) -> float:
        """ğŸ¯ Ã‰TAPE 1: RÃ©compenses focalisÃ©es sur le TIR."""
        reward = 0.0
        
        # OBJECTIF PRINCIPAL : TIRER DES PROJECTILES
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - env.last_projectile_count)
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 10.0  # TRÃˆS gÃ©nÃ©reux
            env.projectiles_fired += projectiles_fired_this_step
        env.last_projectile_count = current_projectile_count
        
        # Bonus pour viser vers les ennemis
        if hasattr(env, 'last_action') and len(env.enemy_spawner.enemies) > 0:
            move_x, move_y, attack_x, attack_y, should_attack = env.last_action
            if should_attack > 0.5:
                # VÃ©rifier si la direction de tir est vers un ennemi
                closest_enemy = min(env.enemy_spawner.enemies, 
                                  key=lambda e: ((e.rect.centerx - env.player.rect.centerx)**2 + 
                                               (e.rect.centery - env.player.rect.centery)**2)**0.5)
                
                # Direction vers l'ennemi
                dx = closest_enemy.rect.centerx - env.player.rect.centerx
                dy = closest_enemy.rect.centery - env.player.rect.centery
                
                # Normaliser
                length = (dx**2 + dy**2)**0.5
                if length > 0:
                    dx /= length
                    dy /= length
                    
                    # Calculer similaritÃ© avec direction de tir
                    dot_product = dx * attack_x + dy * attack_y
                    if dot_product > 0.3:  # 45 degrÃ©s de tolÃ©rance
                        reward += 5.0  # Bonus pour bien viser
        
        # Survie minimale
        reward += 0.1
        
        # PÃ©nalitÃ© mort seulement
        if env.player.health <= 0:
            reward -= 20.0
            
        return reward
    
    def _stage2_reward(self, env) -> float:
        """ï¿½ Ã‰TAPE 2: RÃ©compenses focalisÃ©es sur la VISÃ‰E."""
        reward = 0.0
        
        # OBJECTIF PRINCIPAL : VISER CORRECTEMENT LES ENNEMIS
        if hasattr(env, 'last_action') and len(env.enemy_spawner.enemies) > 0:
            move_x, move_y, attack_x, attack_y, should_attack = env.last_action
            
            # Trouver l'ennemi le plus proche
            closest_enemy = min(env.enemy_spawner.enemies, 
                              key=lambda e: ((e.rect.centerx - env.player.rect.centerx)**2 + 
                                           (e.rect.centery - env.player.rect.centery)**2)**0.5)
            
            # Direction vers l'ennemi
            dx = closest_enemy.rect.centerx - env.player.rect.centerx
            dy = closest_enemy.rect.centery - env.player.rect.centery
            
            # Normaliser
            length = (dx**2 + dy**2)**0.5
            if length > 0:
                dx /= length
                dy /= length
                
                # Calculer similaritÃ© avec direction de tir
                dot_product = dx * attack_x + dy * attack_y
                
                # RÃ©compenser la bonne visÃ©e (mÃªme sans tirer)
                if dot_product > 0.5:  # Bonne direction
                    reward += dot_product * 8.0  # TRÃˆS gÃ©nÃ©reux pour bien viser
                
                # Bonus Ã©norme si tire ET vise bien
                if should_attack > 0.5 and dot_product > 0.3:
                    reward += 12.0  # JACKPOT pour tir bien visÃ©
        
        # Garder l'acquis de l'Ã©tape 1 (tirer) - AUGMENTÃ‰
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - env.last_projectile_count)
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 5.0  # ğŸ”¥ AugmentÃ© de 3.0 Ã  5.0
        env.last_projectile_count = current_projectile_count
        
        # Bonus pour kills (preuve de bonne visÃ©e) - AUGMENTÃ‰
        reward += env.enemies_killed_by_projectiles * 30.0  # ğŸ”¥ AugmentÃ© de 20.0 Ã  30.0
        
        # Survie minimale
        reward += 0.1
        
        # PÃ©nalitÃ© mort
        if env.player.health <= 0:
            reward -= 25.0
            
        return reward
    
    def _stage3_reward(self, env) -> float:
        """ï¿½ğŸƒ Ã‰TAPE 3: RÃ©compenses focalisÃ©es sur le MOUVEMENT."""
        reward = 0.0
        
        # OBJECTIF PRINCIPAL : SE DÃ‰PLACER INTELLIGEMMENT
        if hasattr(env, 'last_action'):
            move_x, move_y, attack_x, attack_y, should_attack = env.last_action
            is_moving = abs(move_x) > 0.1 or abs(move_y) > 0.1
            
            if is_moving:
                reward += 3.0  # Bonus pour bouger
                
                # Bonus pour s'Ã©loigner des ennemis
                if len(env.enemy_spawner.enemies) > 0:
                    closest_enemy = min(env.enemy_spawner.enemies, 
                                      key=lambda e: ((e.rect.centerx - env.player.rect.centerx)**2 + 
                                                   (e.rect.centery - env.player.rect.centery)**2)**0.5)
                    
                    # Direction pour s'Ã©loigner
                    dx = env.player.rect.centerx - closest_enemy.rect.centerx
                    dy = env.player.rect.centery - closest_enemy.rect.centery
                    length = (dx**2 + dy**2)**0.5
                    
                    if length > 0:
                        dx /= length
                        dy /= length
                        
                        # RÃ©compenser mouvement dans la bonne direction
                        dot_product = dx * move_x + dy * move_y
                        if dot_product > 0:
                            reward += dot_product * 2.0
        
        # Garder les acquis prÃ©cÃ©dents (tir et visÃ©e) - AUGMENTÃ‰
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - env.last_projectile_count)
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 4.0  # ğŸ”¥ AugmentÃ© de 2.0 Ã  4.0
        env.last_projectile_count = current_projectile_count
        
        # Bonus pour kills - AJOUTÃ‰
        reward += env.enemies_killed_by_projectiles * 20.0  # ğŸ”¥ Nouveau bonus
        
        # Ã‰viter les bords
        player_x = env.player.rect.centerx
        player_y = env.player.rect.centery
        if (player_x < 50 or player_x > env.screen_width - 50 or 
            player_y < 50 or player_y > env.screen_height - 50):
            reward -= 2.0
            
        # Survie
        reward += 0.2
        
        # PÃ©nalitÃ© dÃ©gÃ¢ts
        health_lost = env.last_player_health - env.player.health
        if health_lost > 0:
            reward -= health_lost * 2.0
            env.last_player_health = env.player.health
        
        # PÃ©nalitÃ© mort
        if env.player.health <= 0:
            reward -= 30.0
            
        return reward
    
    def _stage4_reward(self, env) -> float:
        """ğŸ›¡ï¸ Ã‰TAPE 4: RÃ©compenses focalisÃ©es sur la SURVIE."""
        reward = 0.0
        
        # OBJECTIF PRINCIPAL : SE DÃ‰PLACER INTELLIGEMMENT
        if hasattr(env, 'last_action'):
            move_x, move_y, attack_x, attack_y, should_attack = env.last_action
            is_moving = abs(move_x) > 0.1 or abs(move_y) > 0.1
            
            if is_moving:
                reward += 3.0  # Bonus pour bouger
                
                # Bonus pour s'Ã©loigner des ennemis
                if len(env.enemy_spawner.enemies) > 0:
                    closest_enemy = min(env.enemy_spawner.enemies, 
                                      key=lambda e: ((e.rect.centerx - env.player.rect.centerx)**2 + 
                                                   (e.rect.centery - env.player.rect.centery)**2)**0.5)
                    
                    # Direction pour s'Ã©loigner
                    dx = env.player.rect.centerx - closest_enemy.rect.centerx
                    dy = env.player.rect.centery - closest_enemy.rect.centery
                    length = (dx**2 + dy**2)**0.5
                    
                    if length > 0:
                        dx /= length
                        dy /= length
                        
                        # RÃ©compenser mouvement dans la bonne direction
                        dot_product = dx * move_x + dy * move_y
                        if dot_product > 0:
                            reward += dot_product * 2.0
        
        # Garder les acquis de l'Ã©tape 1 (mais moins important)
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - env.last_projectile_count)
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 2.0  # Moins gÃ©nÃ©reux qu'Ã©tape 1
        env.last_projectile_count = current_projectile_count
        
        # Ã‰viter les bords
        player_x = env.player.rect.centerx
        player_y = env.player.rect.centery
        if (player_x < 50 or player_x > env.screen_width - 50 or 
            player_y < 50 or player_y > env.screen_height - 50):
            reward -= 2.0
            
        # Survie
        reward += 0.2
        
        # PÃ©nalitÃ© dÃ©gÃ¢ts
        health_lost = env.last_player_health - env.player.health
        if health_lost > 0:
            reward -= health_lost * 2.0
            env.last_player_health = env.player.health
        
        # PÃ©nalitÃ© mort
        if env.player.health <= 0:
            reward -= 30.0
            
        return reward
    
    def _stage4_reward(self, env) -> float:
        """ğŸ›¡ï¸ Ã‰TAPE 4: RÃ©compenses focalisÃ©es sur la SURVIE."""
        reward = 0.0
        
        # OBJECTIF PRINCIPAL : SURVIVRE LONGTEMPS
        reward += 0.5  # Bonus de survie augmentÃ©
        
        # Bonus pour kills (combinaison tir + survie)
        reward += env.enemies_killed_by_projectiles * 15.0
        
        # Garder les acquis (tir, visÃ©e, mouvement)
        if hasattr(env, 'last_action'):
            move_x, move_y, attack_x, attack_y, should_attack = env.last_action
            is_moving = abs(move_x) > 0.1 or abs(move_y) > 0.1
            if is_moving:
                reward += 1.0
        
        # TIR MAINTENU - AUGMENTÃ‰
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - env.last_projectile_count)
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 3.0  # ğŸ”¥ AugmentÃ© de 1.0 Ã  3.0
        env.last_projectile_count = current_projectile_count
        
        # Gestion des dÃ©gÃ¢ts (critique pour survie)
        health_lost = env.last_player_health - env.player.health
        if health_lost > 0:
            reward -= health_lost * 5.0
            env.last_player_health = env.player.health
        
        # Position tactique
        player_x = env.player.rect.centerx
        player_y = env.player.rect.centery
        
        # Ã‰viter les bords
        min_distance_to_edge = min(player_x, env.screen_width - player_x, 
                                 player_y, env.screen_height - player_y)
        if min_distance_to_edge < 100:
            reward -= 3.0
        else:
            reward += 0.5
        
        # PÃ©nalitÃ© mort massive
        if env.player.health <= 0:
            reward -= 100.0
            
        return reward
    
    def _stage5_reward(self, env) -> float:
        """ğŸ’ Ã‰TAPE 5: RÃ©compenses AMÃ‰LIORÃ‰ES pour COLLECTE D'ORBES D'XP."""
        reward = 0.0
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # OBJECTIF PRINCIPAL : COLLECTER LES ORBES D'XP
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # 1. RÃ‰COMPENSE MASSIVE POUR COLLECTE
        xp_gained = env.xp_system.current_xp - getattr(env, 'last_xp_count', env.xp_system.current_xp)
        if xp_gained > 0:
            reward += xp_gained * 5.0  # ğŸ”¥ AugmentÃ© de 2.0 Ã  5.0 (250% plus!)
            
            # BONUS COMBO : Collecter plusieurs orbes dans un court laps de temps
            if not hasattr(env, 'orb_collection_streak'):
                env.orb_collection_streak = 0
                env.last_collection_time = 0
            
            # Si collecte rapide (< 100 steps depuis derniÃ¨re)
            if (env.step_count - env.last_collection_time) < 100:
                env.orb_collection_streak += 1
                reward += env.orb_collection_streak * 10.0  # ğŸ”¥ COMBO MULTIPLIER!
            else:
                env.orb_collection_streak = 1
            
            env.last_collection_time = env.step_count
            env.last_xp_count = env.xp_system.current_xp
        
        # 2. RÃ‰COMPENSE POUR PROXIMITÃ‰ AUX ORBES
        if len(env.xp_orbs) > 0:
            player_pos = (env.player.rect.centerx, env.player.rect.centery)
            
            # Trouver l'orbe le plus proche
            closest_orb = min(env.xp_orbs,
                             key=lambda orb: ((orb.x - player_pos[0])**2 + 
                                            (orb.y - player_pos[1])**2)**0.5)
            
            distance = ((closest_orb.x - player_pos[0])**2 + 
                       (closest_orb.y - player_pos[1])**2)**0.5
            
            # ğŸ”¥ BONUS DE PROXIMITÃ‰ (plus proche = meilleur)
            if distance < 150:  # Rayon d'attraction magnÃ©tique
                proximity_bonus = (150 - distance) / 150 * 5.0
                reward += proximity_bonus  # Max +5 si trÃ¨s proche
            
            # 3. RÃ‰COMPENSE POUR SE DIRIGER VERS L'ORBE
            if hasattr(env, 'last_action') and distance > 10:
                # Direction vers l'orbe
                dx = closest_orb.x - player_pos[0]
                dy = closest_orb.y - player_pos[1]
                length = (dx**2 + dy**2)**0.5
                
                if length > 0:
                    dx /= length
                    dy /= length
                    
                    # Mouvement vers l'orbe
                    move_x, move_y, _, _, _ = env.last_action
                    dot_product = dx * move_x + dy * move_y
                    
                    if dot_product > 0.2:  # Plus tolÃ©rant (Ã©tait 0.3)
                        reward += dot_product * 8.0  # ğŸ”¥ AugmentÃ© de 3.0 Ã  8.0
            
            # 4. BONUS POUR NOMBRE D'ORBES DISPONIBLES
            # Plus il y a d'orbes, plus l'IA doit Ãªtre motivÃ©e Ã  les collecter
            num_orbs = len(env.xp_orbs)
            if num_orbs >= 3:
                reward += num_orbs * 0.5  # Incitation Ã  aller nettoyer le terrain
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # GARDER LES ACQUIS (mais avec moins de poids)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # Kills (important car crÃ©ent des orbes!) - AUGMENTÃ‰
        reward += env.enemies_killed_by_projectiles * 25.0  # ğŸ”¥ AugmentÃ© de 15 Ã  25
        
        # Tir (garder le comportement) - AUGMENTÃ‰
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - getattr(env, 'last_projectile_count', 0))
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 2.0  # ğŸ”¥ AugmentÃ© de 0.8 Ã  2.0
        env.last_projectile_count = current_projectile_count
        
        # Survie
        reward += 0.4  # AugmentÃ© de 0.3
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # PÃ‰NALITÃ‰S AJUSTÃ‰ES
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # PÃ©nalitÃ© dÃ©gÃ¢ts RÃ‰DUITE (collecter = risque acceptable)
        health_lost = getattr(env, 'last_player_health', env.player.health) - env.player.health
        if health_lost > 0:
            reward -= health_lost * 2.0  # RÃ‰DUIT de 3.0 Ã  2.0 (risque acceptable)
            env.last_player_health = env.player.health
        
        # PÃ©nalitÃ© mort RÃ‰DUITE (encourager prise de risque calculÃ©e)
        if env.player.health <= 0:
            reward -= 60.0  # RÃ‰DUIT de -80 Ã  -60
            
        return reward
    
    def _stage6_reward(self, env) -> float:
        """ğŸƒ Ã‰TAPE 6: RÃ©compenses focalisÃ©es sur la SÃ‰LECTION DE CARTES."""
        reward = 0.0
        
        # OBJECTIF PRINCIPAL : SÃ‰LECTIONNER DES CARTES STRATÃ‰GIQUEMENT
        # VÃ©rifier si une carte a Ã©tÃ© sÃ©lectionnÃ©e
        if not hasattr(env, 'last_cards_selected'):
            env.last_cards_selected = 0
        
        # Simuler level ups et sÃ©lection de cartes
        current_level = env.xp_system.level
        if not hasattr(env, 'last_level'):
            env.last_level = current_level
        
        if current_level > env.last_level:
            # Level up! RÃ©compense massive
            reward += 50.0
            env.last_cards_selected += 1
            env.last_level = current_level
        
        # RÃ©compenser la collecte d'XP (pour atteindre les level ups)
        xp_gained = env.xp_system.current_xp - getattr(env, 'last_xp_count', env.xp_system.current_xp)
        if xp_gained > 0:
            reward += xp_gained * 1.5
        env.last_xp_count = env.xp_system.current_xp
        
        # Garder tous les acquis - AUGMENTÃ‰
        reward += env.enemies_killed_by_projectiles * 20.0  # ğŸ”¥ AugmentÃ© de 12.0 Ã  20.0
        
        # TIR MAINTENU - AUGMENTÃ‰
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - env.last_projectile_count)
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 2.0  # ğŸ”¥ AugmentÃ© de 0.8 Ã  2.0
        env.last_projectile_count = current_projectile_count
        
        # Survie
        reward += 0.4
        
        # PÃ©nalitÃ© dÃ©gÃ¢ts
        health_lost = env.last_player_health - env.player.health
        if health_lost > 0:
            reward -= health_lost * 4.0
            env.last_player_health = env.player.health
        
        # PÃ©nalitÃ© mort
        if env.player.health <= 0:
            reward -= 100.0
            
        return reward
    
    def _stage7_reward(self, env) -> float:
        """ğŸ† Ã‰TAPE 7: MAÃTRISE COMPLÃˆTE - Survie avec amÃ©liorations."""
        reward = 0.0
        
        # OBJECTIF FINAL : EXCELLENCE DANS TOUS LES DOMAINES
        
        # Survie longue durÃ©e
        reward += 0.6
        
        # Kills avec bonus croissant - AUGMENTÃ‰
        reward += env.enemies_killed_by_projectiles * 30.0  # ğŸ”¥ AugmentÃ© de 20.0 Ã  30.0
        
        # Bonus pour niveau Ã©levÃ© (preuve d'amÃ©liorations)
        current_level = env.xp_system.level
        if current_level >= 5:
            reward += (current_level - 4) * 10.0  # Bonus exponentiel
        
        # Collecte d'XP
        xp_gained = env.xp_system.current_xp - getattr(env, 'last_xp_count', env.xp_system.current_xp)
        if xp_gained > 0:
            reward += xp_gained * 1.0
        env.last_xp_count = env.xp_system.current_xp
        
        # Maintien des compÃ©tences de base - AUGMENTÃ‰
        current_projectile_count = len([p for p in env.player.projectiles if p.active])
        projectiles_fired_this_step = max(0, current_projectile_count - env.last_projectile_count)
        if projectiles_fired_this_step > 0:
            reward += projectiles_fired_this_step * 3.0  # ğŸ”¥ AugmentÃ© de 1.5 Ã  3.0
        env.last_projectile_count = current_projectile_count
        
        # Mouvement tactique
        if hasattr(env, 'last_action'):
            move_x, move_y, _, _, _ = env.last_action
            is_moving = abs(move_x) > 0.1 or abs(move_y) > 0.1
            if is_moving:
                reward += 1.5
        
        # PÃ©nalitÃ© dÃ©gÃ¢ts trÃ¨s forte
        health_lost = env.last_player_health - env.player.health
        if health_lost > 0:
            reward -= health_lost * 8.0
            env.last_player_health = env.player.health
        
        # Position tactique optimale
        player_x = env.player.rect.centerx
        player_y = env.player.rect.centery
        min_distance_to_edge = min(player_x, env.screen_width - player_x, 
                                 player_y, env.screen_height - player_y)
        if min_distance_to_edge < 100:
            reward -= 5.0
        elif min_distance_to_edge > 200:
            reward += 1.0
        
        # PÃ©nalitÃ© mort MASSIVE
        if env.player.health <= 0:
            reward -= 150.0
            
        return reward
    
    def _evaluate_stage(self, stage: int):
        """Ã‰value les performances de l'Ã©tape."""
        print(f"\nğŸ“Š Ã‰VALUATION Ã‰TAPE {stage}")
        print("-" * 40)
        
        # CrÃ©er un environnement simple pour l'Ã©valuation
        from gamepython2d.ai_environment import GameAIEnvironment
        env = GameAIEnvironment(render_mode=None)
        
        metrics = {
            "projectiles_per_episode": [],
            "survival_times": [],
            "movement_scores": [],
            "kills": [],
            "xp_orbs_collected": [],
            "accuracy_scores": [],
            "cards_selected": [],
            "final_levels": []
        }
        
        for episode in range(5):  # RÃ©duire le nombre d'Ã©pisodes pour aller plus vite
            obs, _ = env.reset()
            total_projectiles = 0
            total_movement = 0
            step_count = 0
            kills = 0
            last_projectile_count = 0
            xp_collected = 0
            last_xp = 0
            aimed_shots = 0
            total_shots = 0
            initial_level = env.xp_system.level
            
            while step_count < 1000:  # RÃ©duire la durÃ©e max
                action, _ = self.trainer.model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = env.step(action)
                
                # Mesurer projectiles
                current_projectiles = len([p for p in env.player.projectiles if p.active])
                projectiles_fired = max(0, current_projectiles - last_projectile_count)
                total_projectiles += projectiles_fired
                total_shots += projectiles_fired
                last_projectile_count = current_projectiles
                
                # Mesurer prÃ©cision de visÃ©e (Ã©tape 2)
                if projectiles_fired > 0 and len(env.enemy_spawner.enemies) > 0:
                    attack_x, attack_y = action[2], action[3]
                    closest_enemy = min(env.enemy_spawner.enemies,
                                      key=lambda e: ((e.rect.centerx - env.player.rect.centerx)**2 + 
                                                   (e.rect.centery - env.player.rect.centery)**2)**0.5)
                    dx = closest_enemy.rect.centerx - env.player.rect.centerx
                    dy = closest_enemy.rect.centery - env.player.rect.centery
                    length = (dx**2 + dy**2)**0.5
                    if length > 0:
                        dx /= length
                        dy /= length
                        dot_product = dx * attack_x + dy * attack_y
                        if dot_product > 0.3:
                            aimed_shots += 1
                
                # Mesurer mouvement
                move_x, move_y = action[0], action[1]
                movement = (move_x**2 + move_y**2)**0.5
                total_movement += movement
                
                # Mesurer collecte d'XP
                current_xp = env.xp_system.current_xp
                if current_xp > last_xp:
                    xp_collected += (current_xp - last_xp)
                last_xp = current_xp
                
                step_count += 1
                
                if terminated or truncated:
                    break
            
            metrics["projectiles_per_episode"].append(total_projectiles)
            metrics["survival_times"].append(step_count)
            metrics["movement_scores"].append(total_movement / step_count if step_count > 0 else 0)
            metrics["kills"].append(info.get('enemies_killed_by_projectiles', 0))
            metrics["xp_orbs_collected"].append(xp_collected / 10.0)  # Normaliser
            metrics["accuracy_scores"].append(aimed_shots / total_shots if total_shots > 0 else 0)
            metrics["cards_selected"].append(env.xp_system.level - initial_level)
            metrics["final_levels"].append(env.xp_system.level)
        
        # Afficher rÃ©sultats
        avg_projectiles = np.mean(metrics["projectiles_per_episode"])
        avg_survival = np.mean(metrics["survival_times"])
        avg_movement = np.mean(metrics["movement_scores"])
        avg_kills = np.mean(metrics["kills"])
        avg_xp_collected = np.mean(metrics["xp_orbs_collected"])
        avg_accuracy = np.mean(metrics["accuracy_scores"])
        avg_cards = np.mean(metrics["cards_selected"])
        avg_level = np.mean(metrics["final_levels"])
        
        print(f"ğŸ¯ Projectiles/Ã©pisode: {avg_projectiles:.1f}")
        print(f"ğŸ¨ PrÃ©cision visÃ©e: {avg_accuracy:.1%}")
        print(f"ğŸƒ Score de mouvement: {avg_movement:.3f}")
        print(f"â±ï¸ Temps de survie: {avg_survival:.0f} steps")
        print(f"âš”ï¸ Kills/Ã©pisode: {avg_kills:.1f}")
        print(f"ğŸ’ Orbes XP collectÃ©s: {avg_xp_collected:.1f}")
        print(f"ğŸƒ Cartes obtenues: {avg_cards:.1f}")
        print(f"ğŸ“Š Niveau moyen final: {avg_level:.1f}")
        
        # VÃ©rifier critÃ¨res de passage
        criteria = self.stage_criteria.get(stage, {})
        ready_for_next = True
        
        if "projectiles_per_episode" in criteria:
            if avg_projectiles < criteria["projectiles_per_episode"]:
                ready_for_next = False
                print(f"âŒ CritÃ¨re projectiles non atteint: {avg_projectiles:.1f}/{criteria['projectiles_per_episode']}")
        
        if "accuracy_score" in criteria:
            if avg_accuracy < criteria["accuracy_score"]:
                ready_for_next = False
                print(f"âŒ CritÃ¨re prÃ©cision non atteint: {avg_accuracy:.1%}/{criteria['accuracy_score']:.1%}")
        
        if "movement_score" in criteria:
            if avg_movement < criteria["movement_score"]:
                ready_for_next = False
                print(f"âŒ CritÃ¨re mouvement non atteint: {avg_movement:.3f}/{criteria['movement_score']}")
        
        if "survival_time" in criteria:
            if avg_survival < criteria["survival_time"]:
                ready_for_next = False
                print(f"âŒ CritÃ¨re survie non atteint: {avg_survival:.0f}/{criteria['survival_time']}")
        
        if "xp_orbs_collected" in criteria:
            if avg_xp_collected < criteria["xp_orbs_collected"]:
                ready_for_next = False
                print(f"âŒ CritÃ¨re collecte XP non atteint: {avg_xp_collected:.1f}/{criteria['xp_orbs_collected']}")
        
        if "cards_selected" in criteria:
            if avg_cards < criteria["cards_selected"]:
                ready_for_next = False
                print(f"âŒ CritÃ¨re cartes non atteint: {avg_cards:.1f}/{criteria['cards_selected']}")
        
        if "kills" in criteria:
            if avg_kills < criteria["kills"]:
                ready_for_next = False
                print(f"âŒ CritÃ¨re kills non atteint: {avg_kills:.1f}/{criteria['kills']}")
        
        if ready_for_next and stage < 7:
            print(f"âœ… PRÃŠT POUR L'Ã‰TAPE {stage + 1} !")
        elif stage == 7:
            print("ğŸ† CURRICULUM TERMINÃ‰ !")
        else:
            print("âš ï¸ Besoin d'entraÃ®nement supplÃ©mentaire")
        
        env.close()
        return ready_for_next
    
    def run_full_curriculum(self):
        """Lance l'apprentissage complet par curriculum avec validation stricte."""
        print("\n" + "="*70)
        print("ğŸ“ DÃ‰MARRAGE DU CURRICULUM LEARNING - 7 Ã‰TAPES")
        print("="*70)
        print(f"\nğŸ“Š Configuration:")
        print(f"  â€¢ Ã‰tape 1 (Tir):      {self.stage_timesteps[1]:,} timesteps")
        print(f"  â€¢ Ã‰tape 2 (VisÃ©e):    {self.stage_timesteps[2]:,} timesteps")
        print(f"  â€¢ Ã‰tape 3 (Mouvement): {self.stage_timesteps[3]:,} timesteps")
        print(f"  â€¢ Ã‰tape 4 (Survie):   {self.stage_timesteps[4]:,} timesteps")
        print(f"  â€¢ Ã‰tape 5 (XP):       {self.stage_timesteps[5]:,} timesteps")
        print(f"  â€¢ Ã‰tape 6 (Cartes):   {self.stage_timesteps[6]:,} timesteps")
        print(f"  â€¢ Ã‰tape 7 (MaÃ®trise): {self.stage_timesteps[7]:,} timesteps")
        total_timesteps = sum(self.stage_timesteps.values())
        print(f"\n  ğŸ“ˆ TOTAL: ~{total_timesteps:,} timesteps (~{total_timesteps/1000000:.1f}M)")
        estimated_time = total_timesteps / 7000 / 60  # Estimation: 7000 FPS, conversion en minutes
        print(f"  â±ï¸ Temps estimÃ©: ~{estimated_time:.0f} minutes ({estimated_time/60:.1f}h)")
        print("="*70 + "\n")
        
        start_total = time.time()
        successful_stages = 0
        
        for stage in [1, 2, 3, 4, 5, 6, 7]:
            success = self.train_stage_with_validation(stage)
            if success:
                successful_stages += 1
            
            print(f"\n{'='*70}")
            print(f"ğŸ“Š PROGRESSION: {successful_stages}/{stage} Ã©tapes validÃ©es")
            print(f"{'='*70}\n")
        
        total_time = time.time() - start_total
        
        print("\n" + "ğŸ‰"*35)
        print("ğŸ‰ CURRICULUM LEARNING TERMINÃ‰ !")
        print("ğŸ‰"*35)
        print(f"\nğŸ“Š STATISTIQUES FINALES:")
        print(f"  âœ… Ã‰tapes validÃ©es: {successful_stages}/7")
        print(f"  â±ï¸ Temps total: {total_time/60:.1f} minutes ({total_time/3600:.2f}h)")
        print(f"  ğŸ’¾ ModÃ¨le final: ai_models/curriculum_stage_7")
        print("\n" + "="*70 + "\n")

def main():
    """Point d'entrÃ©e principal."""
    print("ğŸ“ CURRICULUM LEARNING TRAINER - 7 Ã‰TAPES")
    print("Apprentissage progressif :")
    print("  1. ğŸ¯ Tir")
    print("  2. ğŸ¨ VisÃ©e")
    print("  3. ğŸƒ Mouvement")
    print("  4. ğŸ›¡ï¸ Survie")
    print("  5. ğŸ’ Collecte d'orbes XP")
    print("  6. ğŸƒ SÃ©lection de cartes")
    print("  7. ğŸ† MaÃ®trise complÃ¨te")
    print("="*60)
    
    trainer = CurriculumLearningTrainer()
    
    # Demander Ã  l'utilisateur
    choice = input("Voulez-vous lancer le curriculum complet ? (o/n): ").strip().lower()
    
    if choice in ['o', 'oui', 'y', 'yes']:
        trainer.run_full_curriculum()
    else:
        print("Curriculum annulÃ©.")

if __name__ == "__main__":
    main()